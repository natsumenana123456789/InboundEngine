import os
import re
import cv2
import time
import json
import shutil
import argparse
import traceback
import requests
import subprocess
import shutil
import pytesseract
from PIL import Image, ImageFilter, ImageEnhance
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    StaleElementReferenceException,
    NoSuchElementException,
)
from notion_client import Client
from datetime import datetime

# âœ… åºƒå‘Šé™¤å¤–ã€RT/å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ã€æŠ•ç¨¿IDè£œå®Œä»˜ã
AD_KEYWORDS = [
    "r10.to",
    "ãµã‚‹ã•ã¨ç´ç¨",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³",
    "ãŠé‡‘å€Ÿã‚Šã‚‰ã‚Œã‚‹",
    "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¬ãƒ¬ãƒ¼ã‚¸",
    "UNEXT",
    "ã‚¨ã‚³ã‚ªã‚¯",
    "#PR",
    "æ¥½å¤©",
    "Amazon",
    "A8",
    "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    "å‰¯æ¥­",
    "bit.ly",
    "shp.ee",
    "t.co/",
]


def normalize_text(text):
    return text.strip()


def login(driver, target=None):
    if os.path.exists("twitter_cookies.json"):
        print("âœ… Cookieã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º â†’ ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—")
        print("ğŸŒ https://twitter.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­â€¦")
        driver.get("https://twitter.com/")
        driver.delete_all_cookies()
        with open("twitter_cookies.json", "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.get(f"https://twitter.com/{target or TWITTER_USERNAME}")
        return

    print("ğŸ” åˆå›ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹")
    driver.get("https://twitter.com/i/flow/login")
    email_input = WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.NAME, "text"))
    )
    email_input.send_keys(TWITTER_EMAIL)
    email_input.send_keys(Keys.ENTER)
    time.sleep(2)

    try:
        username_input = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "text"))
        )
        username_input.send_keys(TWITTER_USERNAME)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)
    except Exception:
        print("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åå…¥åŠ›ã‚¹ã‚­ãƒƒãƒ—")

    password_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.NAME, "password"))
    )
    password_input.send_keys(TWITTER_PASSWORD)
    password_input.send_keys(Keys.ENTER)
    time.sleep(6)

    cookies = driver.get_cookies()
    with open("twitter_cookies.json", "w") as f:
        json.dump(cookies, f)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ â†’ æŠ•ç¨¿è€…ãƒšãƒ¼ã‚¸ã«é·ç§»")
    driver.get(f"https://twitter.com/{EXTRACT_TARGET}")


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  â† ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=options)


def extract_tweet_id(article):
    href_els = article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
    for el in href_els:
        h = el.get_attribute("href")
        m = re.search(r"/status/(\d+)", h or "")
        if m:
            return m.group(1)
    return None


def ocr_image(image_path):
    try:
        img = Image.open(image_path)
        img = img.convert("L")
        img = img.resize((img.width * 2, img.height * 2))
        img = ImageEnhance.Contrast(img).enhance(2.0)
        img = img.filter(ImageFilter.SHARPEN)
        import numpy as np

        img_np = np.array(img)
        img_np = cv2.medianBlur(img_np, 3)
        _, img_np = cv2.threshold(img_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        img = Image.fromarray(img_np)
        text = pytesseract.image_to_string(img, lang="jpn", config="--oem 1 --psm 6")
        print(f"ğŸ“ OCRç”»åƒ({image_path})çµæœ:\n{text.strip()}")
        if not text.strip() or sum(c.isalnum() for c in text) < 3:
            print(f"âš ï¸ OCRç”»åƒ({image_path})ã§æ–‡å­—åŒ–ã‘ã¾ãŸã¯èªè­˜å¤±æ•—ã®å¯èƒ½æ€§")
        return text.strip()
    except Exception as e:
        print(f"OCRå¤±æ•—({image_path}): {e}")
        return "[OCRã‚¨ãƒ©ãƒ¼]"


def extract_self_replies(driver, username):
    replies = []
    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” extract_self_replies: ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–")
            break

        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")

        def is_quote_reply(article):
            quote_els = article.find_elements(
                By.XPATH,
                ".//*[contains(text(), 'å¼•ç”¨')] | .//*[contains(text(), 'Quote')]",
            )
            quote_struct = article.find_elements(
                By.XPATH, ".//div[contains(@aria-label, 'å¼•ç”¨')]"
            )
            return bool(quote_els or quote_struct)

        for article in articles:
            try:
                handle_el = article.find_element(
                    By.XPATH,
                    ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                )
                handle = handle_el.text.strip()
                if handle.replace("@", "") != username:
                    continue

                if is_quote_reply(article):
                    print("âš ï¸ extract_self_replies: å¼•ç”¨RTå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                reply_text = text_el.text.strip() if text_el and text_el.text else ""

                tweet_id = extract_tweet_id(article)
                if not tweet_id:
                    print("âš ï¸ extract_self_replies: tweet_idãŒå–å¾—ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ç”»åƒãƒ»å‹•ç”»æƒ…å ±ã‚‚å–å¾—
                images = [
                    img.get_attribute("src")
                    for img in article.find_elements(
                        By.XPATH,
                        ".//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                    )
                    if img.get_attribute("src")
                ]
                video_posters = [
                    v.get_attribute("poster")
                    for v in article.find_elements(By.XPATH, ".//video")
                    if v.get_attribute("poster")
                ]

                if reply_text:
                    replies.append(
                        {
                            "id": tweet_id,
                            "text": reply_text,
                            "images": images,
                            "video_posters": video_posters,
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue
    return replies


def is_ad_post(text):
    lowered = text.lower()
    return any(k.lower() in lowered for k in AD_KEYWORDS)


def extract_thread_from_detail_page(driver, tweet_url):
    """
    ã‚¹ãƒ¬ãƒƒãƒ‰è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã™ã¹ã¦ã®æŠ•ç¨¿ã‚’æŠ½å‡ºã™ã‚‹
    å¼•æ•°ã®tweet_urlã‹ã‚‰å„æŠ•ç¨¿ã®URLã‚‚è¨­å®šã™ã‚‹
    """
    print(f"\nğŸ•µï¸ æŠ•ç¨¿ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {tweet_url}")
    driver.get(tweet_url)

    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']//time[@datetime]")
            )
        )
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿è¨˜äº‹ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å–å¾—ã«å¤±æ•—: {e}")
        return []

    if (
        "Something went wrong" in driver.page_source
        or "ã“ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ã¾ã›ã‚“" in driver.page_source
    ):
        print(f"âŒ æŠ•ç¨¿ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {tweet_url}")
        return []

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    tweet_blocks = []
    current_id_from_url = re.sub(r"\D", "", tweet_url.split("/")[-1])

    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")
    print(f"cellInnerDivæ•°: {len(cell_divs)}")
    cell_divs = sorted(cell_divs, key=get_transform_y)

    found_other_user_reply_in_thread = False
    found_show_more_separator = False
    for cell_idx, cell in enumerate(cell_divs):
        if found_other_user_reply_in_thread:
            print(
                f"DEBUG extract_thread_from_detail_page: ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ä»¥é™ã®cellå‡¦ç†ã‚’ä¸­æ–­ã€‚"
            )
            break
        if found_show_more_separator:
            print(
                f"DEBUG extract_thread_from_detail_page: 'ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹' ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚ŒãŸãŸã‚ã€ä»¥é™ã®cellå‡¦ç†ã‚’ä¸­æ–­ã€‚"
            )
            break

        try:
            show_more_elements = cell.find_elements(
                By.XPATH, ".//h2//span[text()='ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹']"
            )
            is_show_more_cell = False
            if show_more_elements:
                for el in show_more_elements:
                    if el.is_displayed():
                        is_show_more_cell = True
                        break

            if is_show_more_cell:
                print(
                    f"DEBUG extract_thread_from_detail_page: 'ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹' cell ã‚’æ¤œå‡º (cell {cell_idx})ã€‚ã“ã®cellã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ¬¡å›ä»¥é™ã®cellå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚"
                )
                found_show_more_separator = True
                continue
        except Exception:
            pass

        articles_in_cell = cell.find_elements(
            By.XPATH, ".//article[@data-testid='tweet']"
        )
        if not articles_in_cell:
            continue

        for article_idx, article in enumerate(articles_in_cell):
            if found_other_user_reply_in_thread:
                break

            tweet_id = None
            username = ""
            try:
                time_links = article.find_elements(
                    By.XPATH, ".//a[.//time[@datetime] and contains(@href, '/status/')]"
                )
                if time_links:
                    href = time_links[0].get_attribute("href")
                    match = re.search(r"/status/(\d{10,})", href)
                    if match:
                        tweet_id = match.group(1)

                if not tweet_id:  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    all_status_links = article.find_elements(
                        By.XPATH, ".//a[contains(@href, '/status/')]"
                    )
                    if all_status_links:
                        for link_el in all_status_links:
                            href = link_el.get_attribute("href")
                            if href:
                                match = re.search(r"/status/(\d{10,})", href)
                                if match:
                                    tweet_id = match.group(1)
                                    break
                    if not tweet_id:
                        # print(f"DEBUG: tweet_id ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã“ã®articleã‚’ã‚¹ã‚­ãƒƒãƒ— (cell {cell_idx}, article {article_idx})")
                        continue

                try:
                    username_el = article.find_element(
                        By.XPATH,
                        ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                    )
                    username = username_el.text.replace("@", "").strip()
                except NoSuchElementException:
                    # print(f"DEBUG: username ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã“ã®articleã‚’ã‚¹ã‚­ãƒƒãƒ— (ID: {tweet_id})")
                    continue

                if not username:
                    # print(f"DEBUG: username ãŒç©ºã®ãŸã‚ã€ã“ã®articleã‚’ã‚¹ã‚­ãƒƒãƒ— (ID: {tweet_id})")
                    continue

                if username.lower() != EXTRACT_TARGET.lower():
                    if tweet_id != current_id_from_url:
                        found_other_user_reply_in_thread = True
                        break

                text = ""
                try:
                    tweet_text_element = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    text_content = driver.execute_script(
                        "return arguments[0].innerText;", tweet_text_element
                    )
                    text = text_content.strip() if text_content else ""
                except NoSuchElementException:
                    text = ""
                except Exception as e_text:
                    print(
                        f"âš ï¸ æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (ID: {tweet_id}): {type(e_text).__name__} - {e_text}"
                    )
                    text = ""

                is_quote_tweet = False
                active_xpath_for_log = "N/A"

                possible_quote_xpaths_for_wait = [
                    ".//div[@data-testid='tweetQuote']",
                    ".//div[contains(@class, 'r-9aw3ui') and .//div[@role='link']]",  # articleãƒã‚¹ãƒˆãªã—ã‚‚è€ƒæ…®
                    ".//div[@aria-labelledby and ./div[@role='link']]",  # articleãƒã‚¹ãƒˆãªã—ã‚‚è€ƒæ…®
                ]
                found_by_wait = False
                wait_time_for_quote = 0.75
                for idx_wait, pq_xpath_wait in enumerate(
                    possible_quote_xpaths_for_wait
                ):
                    try:
                        WebDriverWait(article, wait_time_for_quote).until(
                            EC.presence_of_element_located((By.XPATH, pq_xpath_wait))
                        )
                        found_by_wait = True
                        break
                    except:
                        pass

                try:
                    # å„ªå…ˆåº¦1: data-testid="tweetQuote"
                    xpath_testid = ".//div[@data-testid='tweetQuote']"
                    quote_elements_testid = article.find_elements(
                        By.XPATH, xpath_testid
                    )
                    if quote_elements_testid:
                        is_quote_tweet = True
                        # ãƒã‚¹ãƒˆã•ã‚ŒãŸarticleãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚‚ãƒ­ã‚°ã«ã¯æ®‹ã™
                        if quote_elements_testid[0].find_elements(
                            By.XPATH, ".//article[@data-testid='tweet']"
                        ):
                            active_xpath_for_log = (
                                xpath_testid + " (with nested article)"
                            )
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by testid (with nested article): '{active_xpath_for_log}'"
                            )
                        else:
                            active_xpath_for_log = xpath_testid + " (container only)"
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by testid (container only): '{active_xpath_for_log}'"
                            )
                    else:
                        print(
                            f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements found for XPath (testid): '{xpath_testid}'."
                        )

                    # å„ªå…ˆåº¦1.6: ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ§‹é€  (IDéä¾å­˜)
                    if not is_quote_tweet:
                        # ã‚¯ãƒ©ã‚¹ 'r-9aw3ui' ãŠã‚ˆã³ 'r-1s2bzr4' ã‚’æŒã¡ã€aria-labelledby å±æ€§ã‚’æŒã¡ã€
                        # ã‹ã¤å­è¦ç´ ã« role='link' ã‚’æŒã¤ (ãã®ä¸­ã«articleãŒã‚ã‚‹ã‹ã¯å•ã‚ãªã„)
                        xpath_class_aria_child_link = ".//div[contains(@class, 'r-9aw3ui') and contains(@class, 'r-1s2bzr4') and @aria-labelledby and ./div[@role='link']]"
                        quote_elements_class_aria_child_link = article.find_elements(
                            By.XPATH, xpath_class_aria_child_link
                        )
                        if quote_elements_class_aria_child_link:
                            is_quote_tweet = True
                            active_xpath_for_log = xpath_class_aria_child_link
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by class 'r-9aw3ui', 'r-1s2bzr4', aria-labelledby and child div[@role='link']: '{active_xpath_for_log}'"
                            )
                        else:
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements for XPath (class 'r-9aw3ui', 'r-1s2bzr4', aria, child div[@role='link']): '{xpath_class_aria_child_link}'"
                            )

                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®ãƒã‚¹ãƒˆã•ã‚ŒãŸarticleã‚’æœŸå¾…ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
                            xpath_class_aria_structure = ".//div[contains(@class, 'r-9aw3ui') and contains(@class, 'r-1s2bzr4') and @aria-labelledby and ./div[@role='link' and .//article[@data-testid='tweet']]]"
                            quote_elements_class_aria = article.find_elements(
                                By.XPATH, xpath_class_aria_structure
                            )
                            if quote_elements_class_aria:
                                is_quote_tweet = True
                                active_xpath_for_log = xpath_class_aria_structure
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by class 'r-9aw3ui', 'r-1s2bzr4', aria-labelledby and structure (nested article): '{active_xpath_for_log}'"
                                )
                            else:
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements for XPath (class 'r-9aw3ui', aria, structure with nested article): '{xpath_class_aria_structure}'"
                                )

                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: aria-labelledby ã®æ¡ä»¶ã‚’é™¤ã„ãŸã‚¯ãƒ©ã‚¹ã¨å­div[@role='link']
                                xpath_class_child_link_only = ".//div[contains(@class, 'r-9aw3ui') and contains(@class, 'r-1s2bzr4') and ./div[@role='link']]"
                                quote_elements_class_child_link = article.find_elements(
                                    By.XPATH, xpath_class_child_link_only
                                )
                                if quote_elements_class_child_link:
                                    is_quote_tweet = True
                                    active_xpath_for_log = xpath_class_child_link_only
                                    print(
                                        f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by class 'r-9aw3ui', 'r-1s2bzr4' and child div[@role='link'] (no aria check): '{active_xpath_for_log}'"
                                    )
                                else:
                                    print(
                                        f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements for XPath (class 'r-9aw3ui', child div[@role='link'], no aria): '{xpath_class_child_link_only}'"
                                    )

                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: ä¸»è¦ã‚¯ãƒ©ã‚¹ 'r-9aw3ui' ã¨å­div[@role='link']
                                    xpath_main_class_child_link = ".//div[contains(@class, 'r-9aw3ui') and ./div[@role='link']]"
                                    quote_elements_main_class_child_link = (
                                        article.find_elements(
                                            By.XPATH, xpath_main_class_child_link
                                        )
                                    )
                                    if quote_elements_main_class_child_link:
                                        is_quote_tweet = True
                                        active_xpath_for_log = (
                                            xpath_main_class_child_link
                                        )
                                        print(
                                            f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by main class 'r-9aw3ui' and child div[@role='link']: '{active_xpath_for_log}'"
                                        )
                                    else:
                                        print(
                                            f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements for XPath (main class 'r-9aw3ui' and child div[@role='link']): '{xpath_main_class_child_link}'"
                                        )

                    # å„ªå…ˆåº¦2: aria-labelledby ã‚’æŒã¤è¦ç´  (IDéä¾å­˜ã®ã¾ã¾)
                    if not is_quote_tweet:
                        # ä¿®æ­£: ./div[@role='link'] ã®ä¸­ã® article ã®å­˜åœ¨ã‚’å¿…é ˆã¨ã—ãªã„
                        xpath_aria_child_link = "./descendant::div[@aria-labelledby and ./div[@role='link']]"
                        quote_elements_aria_child_link = article.find_elements(
                            By.XPATH, xpath_aria_child_link
                        )
                        if quote_elements_aria_child_link:
                            is_quote_tweet = True
                            active_xpath_for_log = xpath_aria_child_link
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by aria-labelledby and child div[@role='link']: '{active_xpath_for_log}'"
                            )
                        else:
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements for XPath (aria-labelledby and child div[@role='link']): '{xpath_aria_child_link}'"
                            )
                            # å…ƒã®ãƒã‚¹ãƒˆã•ã‚ŒãŸarticleã‚’æœŸå¾…ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦è©¦ã™
                            xpath_aria_original_nested_article = "./descendant::div[@aria-labelledby][.//article[@data-testid='tweet']]"
                            quote_elements_aria_original_nested = article.find_elements(
                                By.XPATH, xpath_aria_original_nested_article
                            )
                            if quote_elements_aria_original_nested:
                                is_quote_tweet = True
                                active_xpath_for_log = (
                                    xpath_aria_original_nested_article
                                    + " (original logic with nested article)"
                                )
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by original aria-labelledby logic (with nested article): '{active_xpath_for_log}'"
                                )
                            else:
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements found for XPath (aria-labelledby with nested article): '{xpath_aria_original_nested_article}'"
                                )

                    # å„ªå…ˆåº¦3: æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒªãƒ³ã‚¯æ§‹é€ )
                    if not is_quote_tweet:
                        # å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®å­˜åœ¨ã¨ã€ãƒªãƒ³ã‚¯æ§‹é€ ï¼ˆä¸­ã«articleãŒãªãã¦ã‚‚è‰¯ã„ï¼‰
                        xpath_structural_text_and_link = "./descendant::div[(div[normalize-space(text())='å¼•ç”¨' or normalize-space(text())='Quote' or .//span[normalize-space(text())='å¼•ç”¨' or normalize-space(text())='Quote']]) and div[@role='link']]"
                        quote_elements_structural_text_link = article.find_elements(
                            By.XPATH, xpath_structural_text_and_link
                        )
                        if quote_elements_structural_text_link:
                            is_quote_tweet = True
                            active_xpath_for_log = xpath_structural_text_and_link
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by structural fallback (text and link): '{active_xpath_for_log}'"
                            )
                        else:
                            print(
                                f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements found for XPath (structural fallback text and link): '{xpath_structural_text_and_link}'"
                            )
                            # å…ƒã®ãƒã‚¹ãƒˆã•ã‚ŒãŸarticleã‚’æœŸå¾…ã™ã‚‹æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            xpath_structural_nested_article = "./descendant::div[div[normalize-space(text())='å¼•ç”¨' or normalize-space(text())='Quote' or .//span[normalize-space(text())='å¼•ç”¨' or normalize-space(text())='Quote']] and div[@role='link' and .//article[@data-testid='tweet']]]"
                            quote_elements_structural_nested = article.find_elements(
                                By.XPATH, xpath_structural_nested_article
                            )
                            if quote_elements_structural_nested:
                                is_quote_tweet = True
                                active_xpath_for_log = xpath_structural_nested_article
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): Found quote by structural fallback (with nested article): '{active_xpath_for_log}'"
                                )
                            else:
                                print(
                                    f"DEBUG is_quote_tweet check (ID: {tweet_id}): No elements found for XPath (structural fallback with nested article): '{xpath_structural_nested_article}'"
                                )

                    # Specific ID debug logging
                    # if (
                    #     tweet_id == "SPECIFIC_ID_TO_DEBUG"
                    # ) and not is_quote_tweet:
                    #     print(
                    #         f"DEBUG Article innerHTML for {tweet_id} (is_quote_tweet is False after all checks):"
                    #     )
                    #     try:
                    #         inner_html_debug = article.get_attribute("innerHTML")
                    #         print(
                    #             inner_html_debug[:1000] + "..."
                    #             if len(inner_html_debug) > 1000
                    #             else inner_html_debug
                    #         )
                    #     except Exception as e_html_debug_inner:
                    #         print(
                    #             f"DEBUG: Error getting innerHTML for {tweet_id}: {e_html_debug_inner}"
                    #         )

                    print(
                        f"DEBUG is_quote_tweet check (ID: {tweet_id}): Final decision. is_quote_tweet set to {is_quote_tweet} using XPath: '{active_xpath_for_log}'"
                    )

                except NoSuchElementException:
                    print(
                        f"DEBUG is_quote_tweet check (ID: {tweet_id}): NoSuchElementException during quote check."
                    )
                except Exception as e_quote_check_outer:
                    print(
                        f"âš ï¸ is_quote_tweet check outer error (ID: {tweet_id}): {type(e_quote_check_outer).__name__} - {e_quote_check_outer}"
                    )

                images = []
                video_posters = []

                all_possible_image_elements = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media')] | .//div[contains(@data-testid, 'card.layout')]//img[contains(@src, 'twimg.com/card_img')]",
                )
                all_possible_video_elements = article.find_elements(
                    By.XPATH,
                    ".//div[(@data-testid='videoPlayer' or @data-testid='videoComponent' or @data-testid='communitynotesVideo')]//video[@poster]",
                )

                quote_container_element_for_media_check = None
                if is_quote_tweet and active_xpath_for_log != "N/A":
                    try:
                        found_qc = article.find_elements(By.XPATH, active_xpath_for_log)
                        if found_qc:
                            quote_container_element_for_media_check = found_qc[0]
                        else:  # active_xpath_for_log ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            quote_container_xpaths_fallback_media = [
                                ".//div[@data-testid='tweetQuote']",
                                ".//div[contains(@class, 'r-9aw3ui') and @aria-labelledby and ./div[@role='link']]",
                                ".//div[@aria-labelledby and ./div[@role='link']]",
                            ]
                            for (
                                qc_xpath_fb_media
                            ) in quote_container_xpaths_fallback_media:
                                found_qc_fb_media = article.find_elements(
                                    By.XPATH, qc_xpath_fb_media
                                )
                                if found_qc_fb_media:
                                    quote_container_element_for_media_check = (
                                        found_qc_fb_media[0]
                                    )
                                    print(
                                        f"DEBUG Media Check: Used fallback XPath '{qc_xpath_fb_media}' to find quote container for media exclusion."
                                    )
                                    break
                        if not quote_container_element_for_media_check:
                            print(
                                f"DEBUG Media Check: Could not find quote container using active_xpath_for_log ('{active_xpath_for_log}') or fallbacks for media exclusion."
                            )

                    except Exception as e_qc_find:
                        print(
                            f"DEBUG Media Check: Error finding quote container for exclusion using '{active_xpath_for_log}': {e_qc_find}"
                        )

                for img_el in all_possible_image_elements:
                    try:
                        src = img_el.get_attribute("src")
                        if not src:
                            continue

                        is_inside_quote = False
                        if quote_container_element_for_media_check:
                            try:
                                # img_el ãŒ quote_container_element_for_media_check ã®å­å­«è¦ç´ ã§ã‚ã‚‹ã‹ã‚’ç¢ºèª
                                # XPathã§ç›´æ¥ã®å­å­«ã‚’æ¢ã™
                                if quote_container_element_for_media_check.find_elements(
                                    By.XPATH, f".//img[@src='{src}']"
                                ):
                                    is_inside_quote = True
                            except NoSuchElementException:
                                pass
                            except StaleElementReferenceException:
                                print(
                                    f"DEBUG Media Check: Stale quote_container_element_for_media_check for image {src}"
                                )
                                pass

                        if not is_inside_quote:
                            if src not in images:
                                images.append(src)
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue

                for v_el in all_possible_video_elements:
                    try:
                        poster_url = v_el.get_attribute("poster")
                        if not poster_url:
                            continue

                        is_inside_quote_video = False
                        if quote_container_element_for_media_check:
                            try:
                                if quote_container_element_for_media_check.find_elements(
                                    By.XPATH, f".//video[@poster='{poster_url}']"
                                ):
                                    is_inside_quote_video = True
                            except NoSuchElementException:
                                pass
                            except StaleElementReferenceException:
                                print(
                                    f"DEBUG Media Check: Stale quote_container_element_for_media_check for video {poster_url}"
                                )
                                pass

                        if not is_inside_quote_video:
                            if not any(
                                vp.endswith(poster_url.split("/")[-1].split("?")[0])
                                for vp in video_posters
                            ):
                                poster_filename = f"video_poster_{tweet_id}_{len(video_posters)}_{poster_url.split('/')[-1].split('?')[0]}"
                                temp_poster_dir = "temp_posters"
                                if not os.path.exists(temp_poster_dir):
                                    os.makedirs(temp_poster_dir)
                                poster_path = os.path.join(
                                    temp_poster_dir, poster_filename
                                )
                                try:
                                    resp = requests.get(
                                        poster_url, stream=True, timeout=10
                                    )
                                    resp.raise_for_status()
                                    with open(poster_path, "wb") as f_poster:
                                        for chunk in resp.iter_content(1024):
                                            f_poster.write(chunk)
                                    video_posters.append(poster_path)
                                except (
                                    requests.exceptions.RequestException
                                ) as e_poster_dl:
                                    print(
                                        f"âŒ posterç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— (ID: {tweet_id}, URL: {poster_url}): {e_poster_dl}"
                                    )
                                except Exception as e_poster_save:
                                    print(
                                        f"âŒ posterç”»åƒä¿å­˜å¤±æ•— (ID: {tweet_id}): {e_poster_save}"
                                    )
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue

                time_els = article.find_elements(By.XPATH, ".//time[@datetime]")
                date_str = time_els[0].get_attribute("datetime") if time_els else None
                if not date_str:
                    time_links_for_date = article.find_elements(
                        By.XPATH, ".//a[.//time[@datetime]]"
                    )
                    if time_links_for_date:
                        try:
                            date_str = (
                                time_links_for_date[0]
                                .find_element(By.XPATH, ".//time")
                                .get_attribute("datetime")
                            )
                        except:
                            pass

                text_length = len(text)
                has_media = bool(images or video_posters)
                print(
                    f"DEBUG has_media check: ID {tweet_id}, user: {username}, images: {len(images)}, posters: {len(video_posters)}, has_media: {has_media}, is_quote: {is_quote_tweet}"
                )

                tweet_blocks.append(
                    {
                        "article_element": article,
                        "text": text,
                        "date": date_str,
                        "id": tweet_id,
                        "username": username,
                        "images": images,
                        "video_posters": video_posters,
                        "is_quote_tweet": is_quote_tweet,
                        "text_length": text_length,
                        "has_media": has_media,
                    }
                )

            except StaleElementReferenceException:
                print(
                    f"âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿã€ã“ã®articleå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ— (cell {cell_idx}, article {article_idx})"
                )
                break
            except Exception as e_article_process:
                print(
                    f"âš ï¸ è©³ç´°ãƒšãƒ¼ã‚¸å†…è¨˜äº‹å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {type(e_article_process).__name__} - {e_article_process} (ID: {tweet_id if 'tweet_id' in locals() and tweet_id else 'ä¸æ˜'})"
                )
                continue

        if found_other_user_reply_in_thread:
            break

    def remove_temp_posters_from_list(blocks_to_clean):
        for block in blocks_to_clean:
            for poster_p in block.get("video_posters", []):
                if (
                    isinstance(poster_p, str)
                    and os.path.exists(poster_p)
                    and "temp_posters" in poster_p
                ):
                    try:
                        os.remove(poster_p)
                    except Exception as e_remove:
                        print(
                            f"âš ï¸ (cleanup) ä¸€æ™‚ãƒã‚¹ã‚¿ãƒ¼å‰Šé™¤å¤±æ•—: {poster_p}, error: {e_remove}"
                        )

    if not tweet_blocks:
        print(f"âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ (URL: {tweet_url})")
        return []

    initial_post_data = None
    for block in tweet_blocks:
        if block["id"] == current_id_from_url:
            initial_post_data = block
            break

    if not initial_post_data:
        print(
            f"âš ï¸ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ãŒæŠ½å‡ºãƒ–ãƒ­ãƒƒã‚¯å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    if initial_post_data["username"].lower() != EXTRACT_TARGET.lower():
        print(
            f"â„¹ï¸ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼(@{initial_post_data['username']})ãŒå¯¾è±¡({EXTRACT_TARGET})ã¨ç•°ãªã‚Šã¾ã™ãŒã€èµ·ç‚¹ãªã®ã§å‡¦ç†ã¯ç¶™ç¶šã—ã¾ã™ã€‚"
        )

    final_results = []
    for block_item in tweet_blocks:
        if (
            block_item["username"].lower() != EXTRACT_TARGET.lower()
            and block_item["id"] != current_id_from_url
        ):
            remove_temp_posters_from_list([block_item])
            continue

        if is_ad_post(block_item["text"]):
            print(f"ğŸš« åºƒå‘ŠæŠ•ç¨¿ï¼ˆID: {block_item['id']}ï¼‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            remove_temp_posters_from_list([block_item])
            continue

        try:
            impressions, retweets, likes, bookmarks, replies_count = extract_metrics(
                block_item["article_element"]
            )
        except StaleElementReferenceException:
            print(
                f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºä¸­ã«StaleElement (ID: {block_item['id']})ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯0ã«ãªã‚Šã¾ã™ã€‚"
            )
            impressions, retweets, likes, bookmarks, replies_count = None, 0, 0, 0, 0
        except Exception as e_metrics:
            print(
                f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (ID: {block_item['id']}): {e_metrics}ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯0ã«ãªã‚Šã¾ã™ã€‚"
            )
            impressions, retweets, likes, bookmarks, replies_count = None, 0, 0, 0, 0

        final_block = block_item.copy()
        final_block.pop("article_element", None)

        # æŠ•ç¨¿URLã¯ãƒšãƒ¼ã‚¸URLã‹ã‚‰å–å¾—ã™ã‚‹ï¼ˆå®Ÿéš›ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®URLã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ï¼‰
        tweet_url_for_post = tweet_url
        if current_id_from_url != block_item["id"]:
            # ã‚‚ã—ç¾åœ¨ã®ãƒ„ã‚¤ãƒ¼ãƒˆãŒè©³ç´°ãƒšãƒ¼ã‚¸ã®æŠ•ç¨¿ã¨ç•°ãªã‚‹å ´åˆã€
            # è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’ãƒ™ãƒ¼ã‚¹ã«IDã®ã¿ã‚’ç½®ãæ›ãˆã¦æ–°ã—ã„URLã‚’æ§‹ç¯‰
            tweet_url_for_post = tweet_url.replace(
                current_id_from_url, block_item["id"]
            )

        final_block.update(
            {
                "url": tweet_url_for_post,
                "impressions": impressions,
                "retweets": retweets,
                "likes": likes,
                "bookmarks": bookmarks,
                "replies": replies_count,
            }
        )
        final_results.append(final_block)

    if not final_results:
        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµæœã€æœ‰åŠ¹ãªæŠ•ç¨¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        all_blocks_for_cleanup = list(tweet_blocks)
        remove_temp_posters_from_list(all_blocks_for_cleanup)
        return []

    final_results.sort(key=lambda x: int(x["id"]))
    final_ids_in_results = {item["id"] for item in final_results}
    blocks_not_in_final = [
        block for block in tweet_blocks if block["id"] not in final_ids_in_results
    ]
    remove_temp_posters_from_list(blocks_not_in_final)

    return final_results


def extract_and_merge_tweets(
    driver,
    tweet_urls_data,
    max_tweets_to_register,  # ç›®æ¨™ç™»éŒ²æ•°
    notion_client,
    config,
    registered_ids_map,
    current_success_count=0,  # ç¾åœ¨ã®ç™»éŒ²æˆåŠŸæ•°ã‚’è¿½åŠ 
):
    final_tweets_for_notion = []
    # ã“ã®é–¢æ•°ã‚µã‚¤ã‚¯ãƒ«å†…ã§ã®ã¿å‡¦ç†æ¸ˆã¿ã¨ã™ã‚‹ID (ä¸»ã«ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã‚„ã€æ¡ä»¶æœªé”ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸè¦ªå€™è£œãªã©)
    processed_ids_in_current_cycle = set()
    # å…¨ã¦ã®DBã‹ã‚‰å–å¾—ã—ãŸã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å‡¦ç†æ¸ˆã¿ã®IDã‚»ãƒƒãƒˆ
    globally_processed_ids = registered_ids_map.get("all_processed", set())

    # ã“ã®é–¢æ•°å†…ã§åé›†ã™ã‚‹ã€Œå€™è£œã€ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹å¤‰æ•°
    collected_candidates_count = 0

    # ç›®æ¨™ç™»éŒ²æ•°ã«å¯¾ã™ã‚‹å€ç‡ã‚’è¨­å®š
    max_candidates_multiplier = config.get("max_candidates_multiplier", 1.2)

    # æ®‹ã‚Šå¿…è¦ãªç™»éŒ²ä»¶æ•°ã‚’è¨ˆç®—
    remaining_needed = max(0, max_tweets_to_register - current_success_count)

    # æ®‹ã‚Šå¿…è¦æ•°ã«å¯¾ã™ã‚‹å€™è£œåé›†ä¸Šé™ã®è¨ˆç®—
    internal_candidate_collection_limit = min(
        len(tweet_urls_data),
        max(
            int(remaining_needed * max_candidates_multiplier),
            min(15, remaining_needed * 2),  # æ®‹ã‚Šå°‘ãªã„å ´åˆã¯æœ€ä½å€¤ã‚‚èª¿æ•´
        ),
    )

    # URLãƒªã‚¹ãƒˆã‚’IDã®é™é †ï¼ˆæ–°ã—ã„ã‚‚ã®ã‹ã‚‰ï¼‰ã§ã‚½ãƒ¼ãƒˆ
    tweet_urls_data.sort(
        key=lambda x: (
            int(x["id"])
            if isinstance(x, dict) and x.get("id") and str(x["id"]).isdigit()
            else float("-inf")
        ),
        reverse=True,
    )

    print(
        f"â„¹ï¸ extract_and_merge_tweets: é–‹å§‹ã€‚å‡¦ç†å¯¾è±¡URLå€™è£œæ•°: {len(tweet_urls_data)}, "
        f"ç›®æ¨™ç™»éŒ²æ•°: {max_tweets_to_register}, "
        f"ã“ã®é–¢æ•°å†…ã§ã®å€™è£œåé›†ä¸Šé™: {internal_candidate_collection_limit}"
    )

    # äº‹å‰ã«URLã‹ã‚‰IDã‚’æŠ½å‡ºã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
    filtered_urls = []
    print(f"DEBUG: tweet_urls_data first 3 items: {tweet_urls_data[:3]}")
    print(
        f"DEBUG: registered_ids_map['all_processed'] size: {len(registered_ids_map.get('all_processed', set()))}"
    )

    for item in tweet_urls_data:
        # è¾æ›¸å‹ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯ã—ã€é©åˆ‡ã«URLã‚’å–å¾—
        url_str = item["url"] if isinstance(item, dict) else item
        print(f"DEBUG: Processing URL: {url_str}")

        tweet_id_match = re.search(r"/status/(\d+)", url_str)
        if tweet_id_match:
            tweet_id = tweet_id_match.group(1)
            print(f"DEBUG: Extracted ID: {tweet_id}")
            if tweet_id not in registered_ids_map.get("all_processed", set()):
                filtered_urls.append(item)  # å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆè¾æ›¸ã¾ãŸã¯æ–‡å­—åˆ—ï¼‰ã‚’ç¶­æŒ
                print(f"DEBUG: ID {tweet_id} not in processed list, keeping.")
            else:
                print(f"ğŸš« è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å‰ã«é‡è¤‡æ’é™¤: {tweet_id}")
        else:
            print(f"DEBUG: Failed to extract ID from URL: {url_str}")
            filtered_urls.append(item)  # IDãŒæŠ½å‡ºã§ããªã„å ´åˆã‚‚è¿½åŠ 

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿URLãƒªã‚¹ãƒˆã‚’ä½¿ã£ã¦å‡¦ç†ã‚’ç¶šè¡Œ
    tweet_urls_data = filtered_urls

    for i, meta in enumerate(tweet_urls_data):
        if collected_candidates_count >= internal_candidate_collection_limit:
            print(
                f"ğŸ¯ å†…éƒ¨å€™è£œåé›†æ•°ãŒä¸Šé™ ({internal_candidate_collection_limit}) ã«é”ã—ãŸãŸã‚URLå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†"
            )
            break

        tweet_url = meta["url"] if isinstance(meta, dict) else meta
        current_potential_parent_id = meta.get("id") if isinstance(meta, dict) else None

        # ã¾ãšã€æ¸¡ã•ã‚ŒãŸURLã®IDãŒã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã€è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹è‡ªä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if (
            current_potential_parent_id
            and current_potential_parent_id in globally_processed_ids
        ):
            print(
                f"â„¹ï¸ URL {tweet_url} (ID: {current_potential_parent_id}) ã¯æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã€è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            # ã“ã®IDã¯æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ãªã®ã§ã€ã“ã®é–¢æ•°ã® processed_ids_in_current_cycle ã«ã¯è¿½åŠ ä¸è¦
            continue

        try:
            # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®å…¨æŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾— (æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿)
            thread_posts = extract_thread_from_detail_page(driver, tweet_url)
            if not thread_posts:
                print(
                    f"â„¹ï¸ URL {tweet_url} ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                # ã“ã®URLè‡ªä½“ã¯å‡¦ç†è©¦è¡Œã—ãŸãŒçµæœãªã—ã€ã¨ã—ã¦ãƒãƒ¼ã‚¯ã™ã‚‹ã“ã¨ã‚‚æ¤œè¨ã§ãã‚‹
                if current_potential_parent_id:
                    processed_ids_in_current_cycle.add(current_potential_parent_id)
                continue

            parent_post_candidate = None
            # ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç¾åœ¨ã®è¦ªå€™è£œã«ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤IDã‚’ä¸€æ™‚çš„ã«ä¿æŒã™ã‚‹ã‚»ãƒƒãƒˆ
            current_thread_merged_reply_ids = set()

            for post_idx, post_in_thread in enumerate(thread_posts):
                if collected_candidates_count >= internal_candidate_collection_limit:
                    break  # å†…éƒ¨ãƒ«ãƒ¼ãƒ—ã‚‚ä¸Šé™ã«é”ã—ãŸã‚‰æŠœã‘ã‚‹

                current_post_id = post_in_thread.get("id")
                if not current_post_id:
                    print(
                        "âš ï¸ IDãŒãªã„æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ— (in extract_and_merge_tweets)"
                    )
                    continue

                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿IDã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã€ã“ã®æŠ•ç¨¿ã¯å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—
                if current_post_id in globally_processed_ids:
                    print(
                        f"DEBUG: Post ID {current_post_id} is globally processed. Skipping."
                    )
                    continue

                # ã“ã®é–¢æ•°ã‚µã‚¤ã‚¯ãƒ«å†…ã§æ—¢ã«å‡¦ç†æ¸ˆã¿ï¼ˆä¾‹ï¼šå‰ã®è¦ªå€™è£œã«ãƒãƒ¼ã‚¸ã•ã‚ŒãŸã€ã¾ãŸã¯ç™»éŒ²å€™è£œã¨ã—ã¦ç¢ºå®šã—ãŸï¼‰
                # ã‹ã¤ã€ç¾åœ¨ã®è¦ªå€™è£œãã®ã‚‚ã®ã§ãªã„å ´åˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
                if current_post_id in processed_ids_in_current_cycle and (
                    not parent_post_candidate
                    or current_post_id != parent_post_candidate.get("id")
                ):
                    print(
                        f"DEBUG: Post ID {current_post_id} is processed in current cycle and not current parent. Skipping."
                    )
                    continue

                # æ—¢ã« final_tweets_for_notion ã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹IDã‚‚ã‚¹ã‚­ãƒƒãƒ— (å¿µã®ãŸã‚)
                if any(
                    ftn_item["id"] == current_post_id
                    for ftn_item in final_tweets_for_notion
                ):
                    print(
                        f"DEBUG: Post ID {current_post_id} is already in final_tweets_for_notion. Skipping."
                    )
                    continue

                is_current_post_quote = post_in_thread.get("is_quote_tweet", False)
                current_text_len = post_in_thread.get("text_length", 0)
                current_has_media = post_in_thread.get("has_media", False)

                # --- è¦ªå€™è£œãŒã¾ã ãªã„å ´åˆ ---
                if parent_post_candidate is None:
                    # ã“ã®æŠ•ç¨¿ãŒæ¡ä»¶æœªé”ã®å¼•ç”¨RTãªã‚‰ã€å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                    if is_current_post_quote and not (
                        current_text_len >= 50 and current_has_media
                    ):
                        print(
                            f"DEBUG: Initial post {current_post_id} is a non-qualifying quote. Marking as processed."
                        )
                        processed_ids_in_current_cycle.add(current_post_id)
                    else:
                        # ãã‚Œä»¥å¤–ã¯è¦ªå€™è£œã¨ã™ã‚‹
                        parent_post_candidate = post_in_thread.copy()
                        current_thread_merged_reply_ids = (
                            set()
                        )  # æ–°ã—ã„è¦ªå€™è£œãªã®ã§ãƒãƒ¼ã‚¸æ¸ˆã¿IDã‚’ãƒªã‚»ãƒƒãƒˆ
                        print(
                            f"DEBUG: New parent candidate set: {parent_post_candidate.get('id')}"
                        )
                    continue  # æ¬¡ã®æŠ•ç¨¿ã¸

                # --- è¦ªå€™è£œãŒã‚ã‚‹å ´åˆ ---
                # ç¾åœ¨ã®æŠ•ç¨¿ãŒè¦ªå€™è£œè‡ªèº«ãªã‚‰ã‚¹ã‚­ãƒƒãƒ— (é€šå¸¸ã¯èµ·ã“ã‚‰ãªã„ã¯ãšã ãŒå¿µã®ãŸã‚)
                if current_post_id == parent_post_candidate.get("id"):
                    continue

                # ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š: åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã€IDãŒè¦ªå€™è£œã‚ˆã‚Šå°ã•ã„ï¼ˆå¤ã„ï¼‰
                is_reply_to_parent = post_in_thread.get(
                    "username"
                ) == parent_post_candidate.get("username") and int(
                    post_in_thread.get("id", 0)
                ) > int(
                    parent_post_candidate.get("id", 0)
                )

                # --- ãƒªãƒ—ãƒ©ã‚¤ã§ã¯ãªã„å ´åˆ (æ–°ã—ã„è¦ªå€™è£œã®é–‹å§‹) ---
                if not is_reply_to_parent:
                    print(
                        f"DEBUG: Post {current_post_id} is not a reply to current parent {parent_post_candidate.get('id')}. Finalizing current parent."
                    )
                    # ç¾åœ¨ã®è¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ (æ¡ä»¶ã‚’æº€ãŸã›ã°)
                    if parent_post_candidate:
                        # è¦ªå€™è£œãŒã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã§ãªã„ã“ã¨ã‚’å†åº¦ç¢ºèª
                        if (
                            parent_post_candidate.get("id")
                            not in globally_processed_ids
                        ):
                            temp_is_quote = parent_post_candidate.get(
                                "is_quote_tweet", False
                            )
                            temp_text_len = parent_post_candidate.get("text_length", 0)
                            temp_has_media = parent_post_candidate.get(
                                "has_media", False
                            )
                            # æ¡ä»¶ã‚’æº€ãŸã™è¦ªå€™è£œã‹ (æ¡ä»¶æœªé”ã®å¼•ç”¨RTã§ãªã„)
                            if not (
                                temp_is_quote
                                and not (temp_text_len >= 50 and temp_has_media)
                            ):
                                if (
                                    collected_candidates_count
                                    < internal_candidate_collection_limit
                                ):
                                    if not any(
                                        ftn_item["id"]
                                        == parent_post_candidate.get("id")
                                        for ftn_item in final_tweets_for_notion
                                    ):
                                        parent_post_candidate["merged_reply_ids"] = (
                                            list(current_thread_merged_reply_ids)
                                        )
                                        final_tweets_for_notion.append(
                                            parent_post_candidate
                                        )
                                        collected_candidates_count += 1
                                        processed_ids_in_current_cycle.add(
                                            parent_post_candidate.get("id")
                                        )
                                        print(
                                            f"DEBUG: Added parent {parent_post_candidate.get('id')} to final list. Count: {collected_candidates_count}"
                                        )
                                else:  # ä¸Šé™
                                    break
                            else:  # æ¡ä»¶æœªé”ã®å¼•ç”¨RTã ã£ãŸè¦ªå€™è£œ
                                processed_ids_in_current_cycle.add(
                                    parent_post_candidate.get("id")
                                )
                                print(
                                    f"DEBUG: Previous parent {parent_post_candidate.get('id')} was non-qualifying quote. Marked as processed."
                                )
                        else:  # è¦ªå€™è£œãŒã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã ã£ãŸå ´åˆ
                            print(
                                f"DEBUG: Previous parent {parent_post_candidate.get('id')} was globally processed. Not adding to final list."
                            )

                    # æ–°ã—ã„æŠ•ç¨¿ã‚’è¦ªå€™è£œã¨ã—ã¦è¨­å®š
                    if is_current_post_quote and not (
                        current_text_len >= 50 and current_has_media
                    ):
                        parent_post_candidate = None  # æ¡ä»¶æœªé”ã®å¼•ç”¨RTã¯è¦ªå€™è£œã«ã—ãªã„
                        processed_ids_in_current_cycle.add(current_post_id)
                        print(
                            f"DEBUG: New post {current_post_id} is non-qualifying quote. Parent set to None. Marked as processed."
                        )
                    else:
                        parent_post_candidate = post_in_thread.copy()
                        current_thread_merged_reply_ids = (
                            set()
                        )  # æ–°ã—ã„è¦ªå€™è£œãªã®ã§ãƒªã‚»ãƒƒãƒˆ
                        print(
                            f"DEBUG: New parent candidate set: {parent_post_candidate.get('id')}"
                        )
                    continue  # æ¬¡ã®æŠ•ç¨¿ã¸

                # --- è¦ªã¸ã®ãƒªãƒ—ãƒ©ã‚¤ã§ã‚ã‚‹å ´åˆã®å‡¦ç† ---
                print(
                    f"DEBUG: Post {current_post_id} is a reply to parent {parent_post_candidate.get('id')}."
                )
                # ã“ã®ãƒªãƒ—ãƒ©ã‚¤ãŒæ¡ä»¶ã‚’æº€ãŸã™å¼•ç”¨RTã‹ã€ã¾ãŸã¯ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ããƒªãƒ—ãƒ©ã‚¤ã‹
                if (
                    is_current_post_quote
                    and current_text_len >= 50
                    and current_has_media
                ) or (not is_current_post_quote and current_has_media):
                    print(
                        f"DEBUG: Reply {current_post_id} is a qualifying quote or media reply. Finalizing current parent."
                    )
                    # ç¾åœ¨ã®è¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã«è¿½åŠ  (æ¡ä»¶ã‚’æº€ãŸã›ã°)
                    if (
                        parent_post_candidate
                        and parent_post_candidate.get("id")
                        not in globally_processed_ids
                    ):
                        temp_is_quote_parent = parent_post_candidate.get(
                            "is_quote_tweet", False
                        )
                        temp_text_len_parent = parent_post_candidate.get(
                            "text_length", 0
                        )
                        temp_has_media_parent = parent_post_candidate.get(
                            "has_media", False
                        )
                        if not (
                            temp_is_quote_parent
                            and not (
                                temp_text_len_parent >= 50 and temp_has_media_parent
                            )
                        ):
                            if (
                                collected_candidates_count
                                < internal_candidate_collection_limit
                            ):
                                if not any(
                                    ftn_item["id"] == parent_post_candidate.get("id")
                                    for ftn_item in final_tweets_for_notion
                                ):
                                    parent_post_candidate["merged_reply_ids"] = list(
                                        current_thread_merged_reply_ids
                                    )
                                    final_tweets_for_notion.append(
                                        parent_post_candidate
                                    )
                                    collected_candidates_count += 1
                                    processed_ids_in_current_cycle.add(
                                        parent_post_candidate.get("id")
                                    )
                                    print(
                                        f"DEBUG: Added parent {parent_post_candidate.get('id')} to final list due to qualifying reply. Count: {collected_candidates_count}"
                                    )
                            else:  # ä¸Šé™
                                break
                        else:  # æ¡ä»¶æœªé”ã®å¼•ç”¨RTã ã£ãŸè¦ªå€™è£œ
                            processed_ids_in_current_cycle.add(
                                parent_post_candidate.get("id")
                            )
                            print(
                                f"DEBUG: Previous parent {parent_post_candidate.get('id')} was non-qualifying quote. Marked as processed."
                            )

                    # ã“ã®ãƒªãƒ—ãƒ©ã‚¤è‡ªä½“ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã—ã¦è¨­å®š
                    parent_post_candidate = post_in_thread.copy()
                    current_thread_merged_reply_ids = (
                        set()
                    )  # æ–°ã—ã„è¦ªå€™è£œãªã®ã§ãƒªã‚»ãƒƒãƒˆ
                    print(
                        f"DEBUG: Qualifying reply {current_post_id} becomes new parent candidate."
                    )

                elif is_current_post_quote:  # æ¡ä»¶æœªé”ã®å¼•ç”¨ãƒªãƒ—ãƒ©ã‚¤
                    processed_ids_in_current_cycle.add(current_post_id)
                    print(
                        f"DEBUG: Reply {current_post_id} is non-qualifying quote reply. Marked as processed."
                    )
                else:  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒªãƒ—ãƒ©ã‚¤ (ãƒãƒ¼ã‚¸å¯¾è±¡)
                    if parent_post_candidate:
                        parent_text_before_merge = parent_post_candidate.get("text", "")
                        reply_text_to_merge = post_in_thread.get("text", "")
                        # ã‚¹ãƒ¬ãƒƒãƒ‰ã¯æ–°ã—ã„é †ã«å‡¦ç†ã—ã¦ã„ã‚‹ã®ã§ã€å¤ã„ãƒªãƒ—ãƒ©ã‚¤(IDãŒå°ã•ã„)ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Œå‰ã€ã«çµåˆ
                        parent_post_candidate["text"] = (
                            parent_text_before_merge + "\n\n" + reply_text_to_merge
                        ).strip()
                        parent_post_candidate["text_length"] = len(
                            parent_post_candidate["text"]
                        )

                        current_thread_merged_reply_ids.add(
                            current_post_id
                        )  # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤IDã‚’è¨˜éŒ²
                        processed_ids_in_current_cycle.add(
                            current_post_id
                        )  # ã“ã®ã‚µã‚¤ã‚¯ãƒ«ã§ã¯å‡¦ç†æ¸ˆã¿
                        print(
                            f"DEBUG: Merged text-only reply {current_post_id} into parent {parent_post_candidate.get('id')}. Merged IDs: {current_thread_merged_reply_ids}"
                        )

                if collected_candidates_count >= internal_candidate_collection_limit:
                    break  # å†…éƒ¨ãƒ«ãƒ¼ãƒ—ã‚‚ä¸Šé™ã«é”ã—ãŸã‚‰æŠœã‘ã‚‹

            # --- ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®å…¨æŠ•ç¨¿å‡¦ç†å¾Œã€æœ€å¾Œã®è¦ªå€™è£œãŒæ®‹ã£ã¦ã„ã‚Œã°å‡¦ç† ---
            if (
                parent_post_candidate
                and parent_post_candidate.get("id") not in globally_processed_ids
                and not any(
                    ftn_item["id"] == parent_post_candidate.get("id")
                    for ftn_item in final_tweets_for_notion
                )
                and parent_post_candidate.get("id")
                not in processed_ids_in_current_cycle  # ã“ã®ã‚µã‚¤ã‚¯ãƒ«ã§æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ãªã„ã“ã¨ã‚‚ç¢ºèª
            ):
                print(
                    f"DEBUG: Processing final parent candidate {parent_post_candidate.get('id')} after loop."
                )
                is_final_quote = parent_post_candidate.get("is_quote_tweet", False)
                final_text_len = parent_post_candidate.get("text_length", 0)
                final_has_media = parent_post_candidate.get("has_media", False)

                # æ¡ä»¶ã‚’æº€ãŸã™è¦ªå€™è£œã‹ (æ¡ä»¶æœªé”ã®å¼•ç”¨RTã§ãªã„)
                if not (
                    is_final_quote and not (final_text_len >= 50 and final_has_media)
                ):
                    if collected_candidates_count < internal_candidate_collection_limit:
                        parent_post_candidate["merged_reply_ids"] = list(
                            current_thread_merged_reply_ids
                        )
                        final_tweets_for_notion.append(parent_post_candidate)
                        collected_candidates_count += 1
                        processed_ids_in_current_cycle.add(
                            parent_post_candidate.get("id")
                        )
                        print(
                            f"DEBUG: Added final parent {parent_post_candidate.get('id')} to list. Count: {collected_candidates_count}"
                        )
                else:  # æ¡ä»¶æœªé”ã®å¼•ç”¨RTã ã£ãŸæœ€å¾Œã®è¦ªå€™è£œ
                    processed_ids_in_current_cycle.add(parent_post_candidate.get("id"))
                    print(
                        f"DEBUG: Final parent {parent_post_candidate.get('id')} was non-qualifying quote. Marked as processed."
                    )
            elif (
                parent_post_candidate
            ):  # æœ€å¾Œã®è¦ªå€™è£œãŒæ®‹ã£ã¦ã„ãŸãŒã€ä¸Šè¨˜ã®æ¡ä»¶ã§è¿½åŠ ã•ã‚Œãªã‹ã£ãŸå ´åˆ
                # æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã€ã¾ãŸã¯final_tweets_for_notionã«å­˜åœ¨ã€ã¾ãŸã¯ã“ã®ã‚µã‚¤ã‚¯ãƒ«ã§å‡¦ç†æ¸ˆã¿ã®å ´åˆ
                # ã“ã®IDã¯ processed_ids_in_current_cycle ã«è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹æ¤œè¨
                # (é€šå¸¸ã¯ä¸Šè¨˜ã®ifæ¡ä»¶ã®ã„ãšã‚Œã‹ã§æ—¢ã«ãƒãƒ¼ã‚¯ã•ã‚Œã¦ã„ã‚‹ã¯ãš)
                if (
                    parent_post_candidate.get("id")
                    not in processed_ids_in_current_cycle
                    and parent_post_candidate.get("id") not in globally_processed_ids
                    and not any(
                        ftn_item["id"] == parent_post_candidate.get("id")
                        for ftn_item in final_tweets_for_notion
                    )
                ):
                    # ã“ã®ã‚±ãƒ¼ã‚¹ã¯ç¨€ã ãŒã€ã‚‚ã—ç™ºç”Ÿã—ãŸã‚‰ãƒ­ã‚°ã§ç¢ºèª
                    print(
                        f"DEBUG: Final parent candidate {parent_post_candidate.get('id')} was not added and not explicitly marked processed. This might be an edge case."
                    )
                else:
                    print(
                        f"DEBUG: Final parent candidate {parent_post_candidate.get('id')} was skipped (globally processed, already in final list, or processed in cycle)."
                    )

            if collected_candidates_count >= internal_candidate_collection_limit:
                break  # URLå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        except Exception as e:
            print(
                f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ ({tweet_url}): {type(e).__name__} - {e}\n{traceback.format_exc()}"
            )
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚ã€ä¸€æ™‚ä¿å­˜ã•ã‚ŒãŸãƒã‚¹ã‚¿ãƒ¼ç”»åƒã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹è©¦ã¿
            if "thread_posts" in locals() and thread_posts:
                for post_data_item in thread_posts:
                    for poster_p in post_data_item.get("video_posters", []):
                        if (
                            isinstance(poster_p, str)
                            and os.path.exists(poster_p)
                            and "temp_posters" in poster_p
                        ):
                            try:
                                os.remove(poster_p)
                            except Exception:
                                pass
            # ã“ã®URLã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸå ´åˆã€ãã®URLã®IDã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯ã™ã‚‹
            if current_potential_parent_id:
                processed_ids_in_current_cycle.add(current_potential_parent_id)
            continue  # æ¬¡ã®URLã¸

    print(
        f"\nğŸ“ˆ extract_and_merge_tweets: åé›†ã—ãŸå€™è£œæŠ•ç¨¿æ•°: {len(final_tweets_for_notion)} ä»¶"
    )
    # æœ€çµ‚çš„ãªãƒªã‚¹ãƒˆã¯IDã®é™é †ï¼ˆæ–°ã—ã„ã‚‚ã®ãŒå…ˆé ­ï¼‰ã§è¿”ã™
    final_tweets_for_notion.sort(
        key=lambda x: (
            int(x["id"]) if x.get("id") and str(x["id"]).isdigit() else float("-inf")
        ),
        reverse=True,
    )
    return final_tweets_for_notion, processed_ids_in_current_cycle


def extract_metrics(article):
    """
    ã„ã„ã­æ•°ãƒ»ãƒªãƒã‚¹ãƒˆæ•°ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ»ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒ»ãƒªãƒ—ãƒ©ã‚¤æ•°ã‚’æŠ½å‡º
    å–å¾—ã§ããªã„ã‚‚ã®ã¯0ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿Noneï¼‰ã§è¿”ã™
    """
    impressions_str = retweets_str = likes_str = bookmarks_str = replies_str = None
    try:
        # å„ªå…ˆçš„ã« div[role="group"] ã® aria-label ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        # ã“ã‚ŒãŒæœ€ã‚‚æƒ…å ±ãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
        group_divs = article.find_elements(
            By.XPATH, ".//div[@role='group' and @aria-label]"
        )

        primary_label_processed = False
        if group_divs:
            for group_div in group_divs:
                label = group_div.get_attribute("aria-label")
                if not label:
                    continue

                print(f"ğŸŸ¦ metrics group aria-labelå†…å®¹: {label}")
                primary_label_processed = True  # ã“ã®ãƒ©ãƒ™ãƒ«ã‚’å‡¦ç†ã—ãŸã“ã¨ã‚’ãƒãƒ¼ã‚¯

                # å„æŒ‡æ¨™ã‚’å€‹åˆ¥ã«æŠ½å‡ºã™ã‚‹ (é †ç•ªã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«)
                m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                if m_replies:
                    replies_str = m_replies.group(1)

                m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                if m_retweets:
                    retweets_str = m_retweets.group(1)

                m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                if m_likes:
                    likes_str = m_likes.group(1)

                m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                if m_bookmarks:
                    bookmarks_str = m_bookmarks.group(1)

                m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                if m_impressions:
                    impressions_str = m_impressions.group(1)

                # ä¸€ã¤ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰å…¨ã¦å–ã‚ŒãŸã‚‰æŠœã‘ã‚‹ã“ã¨ãŒå¤šã„ãŒã€ç¨€ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ã€
                # åŸºæœ¬çš„ã«ã¯æœ€åˆã® group_div ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸»ã¨ã™ã‚‹ã€‚
                # ã‚‚ã—ã€è¤‡æ•°ã® group_div ãŒç•°ãªã‚‹æƒ…å ±ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒç¢ºèªã•ã‚Œã‚Œã°ã€ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯å†è€ƒã€‚
                break

        if not primary_label_processed:
            # group_div ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€aria-label ãŒãªã„å ´åˆã€ä»¥å‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚è©¦ã™
            # ãŸã ã—ã€ã“ã®ãƒ‘ã‚¹ã¯Xã®UIãŒå¤§ããå¤‰ã‚ã£ãŸå ´åˆã¯æ©Ÿèƒ½ã—ãªã„å¯èƒ½æ€§ãŒé«˜ã„
            other_divs = article.find_elements(
                By.XPATH,
                ".//div[contains(@aria-label, 'ä»¶ã®è¡¨ç¤º') and not(@role='group')]",
            )
            for div in other_divs:
                label = div.get_attribute("aria-label")
                if not label:
                    continue
                print(f"ğŸŸ¦ other metrics div aria-labelå†…å®¹: {label}")
                # ã“ã“ã§ã‚‚åŒæ§˜ã«å€‹åˆ¥æŠ½å‡ºã‚’è©¦ã¿ã‚‹ (ä¸Šè¨˜ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
                if replies_str is None:
                    m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                    if m_replies:
                        replies_str = m_replies.group(1)
                if retweets_str is None:
                    m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                    if m_retweets:
                        retweets_str = m_retweets.group(1)
                if likes_str is None:
                    m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                    if m_likes:
                        likes_str = m_likes.group(1)
                if bookmarks_str is None:
                    m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                    if m_bookmarks:
                        bookmarks_str = m_bookmarks.group(1)
                if impressions_str is None:
                    m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                    if m_impressions:
                        impressions_str = m_impressions.group(1)
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã§å‡¦ç†

        # å€‹åˆ¥ãƒœã‚¿ãƒ³ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—
        if replies_str is None:
            try:
                reply_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='reply']"
                )
                for btn in reply_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label or "")
                    if m:
                        replies_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤æ•°å–å¾—: {replies_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if retweets_str is None:
            try:
                rt_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='retweet']"
                )
                for btn in rt_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label or "")
                    if m:
                        retweets_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒã‚¹ãƒˆæ•°å–å¾—: {retweets_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒã‚¹ãƒˆæ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if likes_str is None:
            try:
                like_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='like']"
                )
                for btn in like_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label or "")
                    if m:
                        likes_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ã„ã„ã­æ•°å–å¾—: {likes_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ã„ã„ã­æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if bookmarks_str is None:
            try:
                bm_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='bookmark']"
                )
                for btn in bm_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label or "")
                    if m:
                        bookmarks_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—: {bookmarks_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã¯ãƒœã‚¿ãƒ³ã‹ã‚‰ã¯é€šå¸¸å–ã‚Œãªã„ã®ã§ã€aria-labelé ¼ã¿
        # ã‚‚ã— impressions_str ãŒ None ã§ã€ä»–ã®æŒ‡æ¨™ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆã€
        # ã‹ã¤ã¦ã®ã€Œã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€
        # likes/retweets/bookmarks/replies ãŒå…¨ã¦0ãªã‚‰ã€impressions_str ã‚’æ¡ç”¨ã—ä»–ã‚’0ã«ã™ã‚‹ã€‚
        # ãŸã ã—ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯è¤‡é›‘ãªã®ã§ã€ä¸€æ—¦ã¯ä¸Šè¨˜ã§å–å¾—ã§ããŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
        # ã‚‚ã—ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã ã‘ãŒå–ã‚Œã¦ä»–ãŒ0ã«ãªã‚‹ã¹ãã‚±ãƒ¼ã‚¹ãŒå¤šç™ºã™ã‚‹ãªã‚‰å†æ¤œè¨ã€‚

        def parse_num(s):
            if not s:
                return 0  # None ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯0ã¨ã—ã¦æ‰±ã† (ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ä»¥å¤–)
            s_cleaned = str(s).replace(",", "")
            if "ä¸‡" in s_cleaned:
                try:
                    return int(float(s_cleaned.replace("ä¸‡", "")) * 10000)
                except ValueError:
                    return 0  # "ä¸‡" ãŒã‚ã£ã¦ã‚‚æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆ
            try:
                return int(s_cleaned)
            except ValueError:  # "K" ã‚„ "M" ãªã©ã®è‹±èªåœã®ç•¥ç§°ã¯ç¾çŠ¶éå¯¾å¿œ
                return 0  # æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆã¯0

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ None ã‚’è¨±å®¹ã—ã€ä»–ã¯0ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
        impressions = (
            parse_num(impressions_str) if impressions_str is not None else None
        )
        retweets = parse_num(retweets_str)
        likes = parse_num(likes_str)
        bookmarks = parse_num(bookmarks_str)
        replies = parse_num(replies_str)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœ€çµ‚çš„ãªå€¤ã‚’è¡¨ç¤º
        print(
            f"ğŸ”¢ æŠ½å‡ºçµæœ: è¡¨ç¤º={impressions}, RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, ãƒªãƒ—ãƒ©ã‚¤={replies}"
        )

    except Exception as e:
        print(f"âš ï¸ extract_metricså…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (impressions=None, ä»–=0)
        impressions = None
        retweets = 0
        likes = 0
        bookmarks = 0
        replies = 0

    return impressions, retweets, likes, bookmarks, replies


def is_reply_structure(
    article,
    tweet_id=None,
    text="",
    image_urls=None,
    video_poster_urls=None,
):
    try:
        id_display = f"ï¼ˆID={tweet_id}ï¼‰" if tweet_id else ""

        try:
            if not article.is_displayed():
                print(
                    f"DEBUG is_reply_structure: Article element not displayed {id_display} -> Assuming reply structure (True)."
                )
                return True
        except StaleElementReferenceException:
            print(
                f"DEBUG is_reply_structure: StaleElementReferenceException on article.is_displayed() {id_display} -> Assuming reply structure (True)."
            )
            return True  # Safety measure

        # å¼•ç”¨RTæ§‹é€ ã®åˆ¤å®š
        is_quote_tweet_structure = False
        try:
            # å„ªå…ˆåº¦1: data-testid="tweetQuote" ã‚’æŒã¤è¦ç´ ãŒã‚ã‚‹ã‹
            quote_testid_elements = article.find_elements(
                By.XPATH, ".//div[@data-testid='tweetQuote']"
            )
            if quote_testid_elements:
                if any(el.is_displayed() for el in quote_testid_elements):
                    is_quote_tweet_structure = True
                    print(
                        f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> data-testid='tweetQuote' ã‚’æ¤œå‡º (è¡¨ç¤ºç¢ºèªæ¸ˆã¿) {id_display}"
                    )
                # else:
                #     print(
                #         f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> data-testid='tweetQuote' ã‚’æ¤œå‡ºã—ãŸãŒéè¡¨ç¤º {id_display}"
                #     )

            # å„ªå…ˆåº¦2: å¾“æ¥ã®æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š (is_quote_tweet_structure ãŒã¾ã  False ã®å ´åˆ)
            if not is_quote_tweet_structure:
                # ä¿®æ­£: å¼•ç”¨RTã®ã‚³ãƒ³ãƒ†ãƒŠã¯å¿…ãšã—ã‚‚å†…éƒ¨ã«articleã‚’æŒã¤ã¨ã¯é™ã‚‰ãªã„ã€‚
                # role='link' ã‚’æŒã¡ã€ãã®ä¸­ã«ä½•ã‚‰ã‹ã®ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’ç¤ºå”†ã™ã‚‹è¦ç´ ãŒã‚ã‚‹ã‹ã€
                # ã¾ãŸã¯ç‰¹å®šã®ã‚¯ãƒ©ã‚¹æ§‹é€ ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™ã€‚
                # ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªã®ã¯ã€"å¼•ç”¨"ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆã‚„ã€ç‰¹å®šã®aria-labelã‚’æŒã¤è¦ç´ ã‚’æ¢ã™ã“ã¨ã€‚

                # æ§‹é€ çš„XPath: role='link' ã‚’æŒã¡ã€ãã®ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŒã¤è¦ç´ ãŒã‚ã‚‹ã‹
                # (ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯å¼•ç”¨ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆã®å­˜åœ¨ã‚’ç¤ºå”†ã™ã‚‹)
                structural_quote_elements = article.find_elements(
                    By.XPATH, "./descendant::div[@role='link' and .//time[@datetime]]"
                )
                if structural_quote_elements:
                    if any(el.is_displayed() for el in structural_quote_elements):
                        # ã“ã‚ŒãŒæœ¬å½“ã«å¼•ç”¨RTã®ã‚³ãƒ³ãƒ†ãƒŠã‹ã€ã•ã‚‰ã«çµã‚Šè¾¼ã‚€å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„
                        # ä¾‹ãˆã°ã€ã“ã®div[@role='link']ã®ç›´å‰ã«ã€Œå¼•ç”¨ã€ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ã‹ãªã©
                        # ä»Šå›ã¯ã€è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹role='link' with timeãŒã‚ã‚Œã°å¼•ç”¨æ§‹é€ ã¨ã¿ãªã™
                        is_quote_tweet_structure = True
                        print(
                            f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> æ§‹é€ çš„XPath (role='link' with time) ãŒ {len(structural_quote_elements)} ä»¶ãƒãƒƒãƒ (è¡¨ç¤ºç¢ºèªæ¸ˆã¿) {id_display}"
                        )
                    # else:
                    #     print(
                    #         f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> æ§‹é€ çš„XPath (role='link' with time) ãŒ {len(structural_quote_elements)} ä»¶ãƒãƒƒãƒã—ãŸãŒéè¡¨ç¤º {id_display}"
                    #     )

            # å„ªå…ˆåº¦3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿)
            if not is_quote_tweet_structure:
                # "å¼•ç”¨" ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒã€ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡(@data-testid='tweetText')ã®å¤–å´ã«ã‚ã‚‹ã‹
                # ã‹ã¤ã€ãã‚ŒãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹
                quote_indicators = article.find_elements(
                    By.XPATH,
                    ".//div[not(ancestor-or-self::div[@data-testid='tweetText'])]//span[text()='å¼•ç”¨']",
                )
                if any(el.is_displayed() for el in quote_indicators):
                    is_quote_tweet_structure = True
                    print(
                        f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> å¼•ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆ'å¼•ç”¨' spanï¼‰ã‚’æ¤œå‡º {id_display}"
                    )

            # if not is_quote_tweet_structure:
            #     print(
            #         f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®š -> å¼•ç”¨RTæ§‹é€ ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ {id_display}"
            #     )

        except StaleElementReferenceException as e_quote_staleness:
            print(
                f"DEBUG is_reply_structure: å¼•ç”¨åˆ¤å®šä¸­ã«StaleElement {id_display} -> {e_quote_staleness}"
            )
            is_quote_tweet_structure = (
                False  # Staleæ™‚ã¯å¼•ç”¨ã§ãªã„ã¨åˆ¤æ–­ã—ã¦é€²ã‚€æ–¹ãŒå®‰å…¨ã‹ã€ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ã‹
            )
        except Exception as e_quote_check:
            print(
                f"âš ï¸ is_reply_structure: å¼•ç”¨åˆ¤å®šä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_quote_check).__name__}: {e_quote_check}"
            )
            is_quote_tweet_structure = False

        # --- å¼•ç”¨RTæ§‹é€ ã®å ´åˆã®å‡¦ç† ---
        if is_quote_tweet_structure:
            text_length = len(text.strip()) if text else 0
            # image_urls ã¨ video_poster_urls ã¯ã€å‘¼ã³å‡ºã—å…ƒ(extract_tweets)ã§
            # ã€Œå¼•ç”¨RTã®æœ¬ä½“ãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã¿ã€ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹å‰æã€‚
            has_direct_media = bool(image_urls or video_poster_urls)

            if text_length >= 50 and has_direct_media:
                print(
                    f"âœ… is_reply_structure: å¼•ç”¨RTï¼ˆæ¡ä»¶é”æˆ: 50æ–‡å­—ä»¥ä¸Šã‹ã¤æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ãï¼‰â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ (False) {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={has_direct_media}"
                )
                return False  # åé›†å¯¾è±¡ã®å¼•ç”¨RT (False = ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ã§ã¯ãªã„)
            else:
                print(
                    f"ğŸ›‘ is_reply_structure: å¼•ç”¨RTï¼ˆæ¡ä»¶æœªé”: 50æ–‡å­—ä»¥ä¸Šã‹ã¤æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ãã§ã¯ãªã„ï¼‰â†’ é™¤å¤– (True) {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={has_direct_media}"
                )
                return True  # åé›†å¯¾è±¡å¤–ã®å¼•ç”¨RT (True = ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ã§ã‚ã‚‹ã€ã¾ãŸã¯åé›†å¯¾è±¡å¤–)

        # --- å¼•ç”¨RTã§ãªã„å ´åˆã®é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š ---

        # 1. ãƒ¡ãƒ‡ã‚£ã‚¢ã‚«ãƒ¼ãƒ‰ã‚„ç›´æ¥ãƒ¡ãƒ‡ã‚£ã‚¢ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        try:
            # data-testid="card.wrapper" ã‚’æŒã¤è¦ç´ ãŒã‚ã‚‹ã‹ (å¼•ç”¨RTå†…éƒ¨ã¯é™¤å¤–)
            card_wrapper_elements = article.find_elements(
                By.XPATH,
                ".//div[@data-testid='card.wrapper'][not(ancestor::div[@data-testid='tweetQuote'])]",
            )
            if card_wrapper_elements:
                if any(el.is_displayed() for el in card_wrapper_elements):
                    print(
                        f"âœ… is_reply_structure: card.wrapper ã‚’æ¤œå‡º (è¡¨ç¤ºç¢ºèªæ¸ˆã¿) â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ (False) {id_display}"
                    )
                    return False
                # else:
                #     print(
                #         f"DEBUG is_reply_structure: card.wrapper ã‚’æ¤œå‡ºã—ãŸãŒéè¡¨ç¤º {id_display}"
                #     )

            # å‘¼ã³å‡ºã—å…ƒã‹ã‚‰æ¸¡ã•ã‚ŒãŸã€Œæœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ã€ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            if image_urls or video_poster_urls:
                print(
                    f"âœ… is_reply_structure: å¼•æ•°ã«ã‚ˆã‚‹ç›´æ¥ãƒ¡ãƒ‡ã‚£ã‚¢æ¤œå‡º (images: {bool(image_urls)}, videos: {bool(video_poster_urls)}) â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ (False) {id_display}"
                )
                return False

        except StaleElementReferenceException:
            print(
                f"DEBUG is_reply_structure: StaleElementReferenceException during card/media check {id_display}."
            )
        except Exception as e_media_check:
            print(
                f"âš ï¸ is_reply_structure: card/media ãƒã‚§ãƒƒã‚¯ä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_media_check).__name__}: {e_media_check}"
            )

        # 2. socialContext (è¿”ä¿¡å…ˆã€å›ºå®šãƒ„ã‚¤ãƒ¼ãƒˆãªã©) ã®ãƒã‚§ãƒƒã‚¯
        is_pinned_tweet = False
        try:
            social_context_elements = article.find_elements(
                By.XPATH, ".//div[@data-testid='socialContext']"
            )
            # if not social_context_elements:
            #     print(
            #         f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            #     )

            for sc_el in social_context_elements:
                if sc_el.is_displayed():
                    sc_text_content = sc_el.text
                    sc_text_lower = sc_text_content.lower()
                    # print(
                    #     f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ: '{sc_text_content}'"
                    # )
                    if "å›ºå®š" in sc_text_content or "pinned" in sc_text_lower:
                        is_pinned_tweet = True
                        # print(
                        #     f"DEBUG is_reply_structure: ID {tweet_id}, socialContextã«ã‚ˆã‚Šå›ºå®šãƒ„ã‚¤ãƒ¼ãƒˆã¨åˆ¤å®šã€‚"
                        # )

                    reply_keywords = ["replying to", "è¿”ä¿¡å…ˆ:", "replied to"]
                    if any(
                        keyword in sc_text_lower for keyword in reply_keywords
                    ) or re.search(r"@\w+\s*ã«è¿”ä¿¡", sc_text_content, re.IGNORECASE):
                        try:
                            # socialContextãŒå¼•ç”¨RTã®ä¸€éƒ¨ã§ã‚ã‚‹ã‹ã‚’ç¢ºèª
                            sc_el.find_element(
                                By.XPATH,
                                "ancestor::div[@data-testid='tweetQuote'] | ancestor::div[@role='link' and .//time[@datetime]]",  # å¼•ç”¨RTã‚³ãƒ³ãƒ†ãƒŠã®åˆ¤å®šã‚’å°‘ã—åºƒã’ã‚‹
                            )
                            # print(
                            #     f"DEBUG is_reply_structure (socialContext): ID {tweet_id}, socialContextã¯å¼•ç”¨RTå†…ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: '{sc_text_content[:30]}...'"
                            # )
                            continue  # å¼•ç”¨RTå†…ãªã‚‰ã“ã®socialContextã¯ç„¡è¦–
                        except NoSuchElementException:
                            # å¼•ç”¨RTå†…ã§ãªã‘ã‚Œã°ã€ãƒªãƒ—ãƒ©ã‚¤ã¨åˆ¤å®š
                            print(
                                f"ğŸ’¬ is_reply_structure (socialContext): ID {tweet_id} â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š (True) (ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´: '{sc_text_content[:30]}...')"
                            )
                            return True
        except StaleElementReferenceException:
            print(
                f"DEBUG is_reply_structure: StaleElementReferenceException during socialContext check {id_display}."
            )
        except NoSuchElementException:
            # print(
            #     f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¦ç´ ã®æ¤œç´¢ã§äºˆæœŸã›ã¬NoSuchElementã€‚"
            # )
            pass
        except Exception as e_sc_check:
            print(
                f"âš ï¸ is_reply_structure: socialContextç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_sc_check).__name__}: {e_sc_check}"
            )

        # 3. æ§‹é€ çš„ãªãƒªãƒ—ãƒ©ã‚¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆãƒªãƒ—ãƒ©ã‚¤ç·šãªã©ï¼‰ã®ãƒã‚§ãƒƒã‚¯
        #    Xã®UIå¤‰æ›´ã§ãƒªãƒ—ãƒ©ã‚¤ç·šã®ã‚¯ãƒ©ã‚¹åã¯éå¸¸ã«ä¸å®‰å®šãªãŸã‚ã€ã“ã®ãƒã‚§ãƒƒã‚¯ã¯é™å®šçš„ã«ã™ã‚‹ã‹ã€
        #    ã‚ˆã‚Šå …ç‰¢ãªæ–¹æ³•ãŒè¦‹ã¤ã‹ã‚‹ã¾ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚‚æ¤œè¨ã€‚
        #    ç¾çŠ¶ã¯ã€ç‰¹å®šã®ã‚¯ãƒ©ã‚¹åã«ä¾å­˜ã—ãªã„ã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªæ§‹é€ ã‚’æ¢ã™æ–¹ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚
        #    ä¾‹ãˆã°ã€ã€Œç‰¹å®šã®è¦ç´ ã®ç›´å‰ã«ç¸¦ç·šã®ã‚ˆã†ãªdivãŒã‚ã‚‹ã€ãªã©ã€‚
        #    ã“ã“ã§ã¯ä¸€æ—¦ã€ä»¥å‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€ã‚ˆã‚Šå®‰å…¨ãªåˆ¤å®šã«å€’ã™ã€‚
        #
        # try:
        #     # ... (ãƒªãƒ—ãƒ©ã‚¤ç·šãƒã‚§ãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯) ...
        # except Exception as e_reply_line_check:
        #     # ...
        #     pass

        # 4. "è¿”ä¿¡å…ˆ: @" ã‚„ "Replying to @" ã®ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã‚„å¼•ç”¨RTã®å¤–å´ã«ã‚ã‚‹ã‹
        try:
            base_condition = "starts-with(normalize-space(.), 'Replying to @') or starts-with(normalize-space(.), 'è¿”ä¿¡å…ˆ: @') or starts-with(normalize-space(.), 'In reply to @')"
            not_in_quote_condition = "not(ancestor::div[@data-testid='tweetQuote']) and not(ancestor::div[@role='link' and .//time[@datetime]])"
            not_in_text_div_condition = (
                "not(ancestor-or-self::div[@data-testid='tweetText'])"
            )

            xpath_for_reply_text_indicator = f".//*[(self::div or self::span) and ({base_condition}) and ({not_in_quote_condition}) and ({not_in_text_div_condition})]"

            reply_to_user_text_elements = article.find_elements(
                By.XPATH, xpath_for_reply_text_indicator
            )

            # if not reply_to_user_text_elements:
            #     print(
            #         f"DEBUG is_reply_structure: ID {tweet_id}, è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (XPath: {xpath_for_reply_text_indicator})"
            #     )

            for el in reply_to_user_text_elements:
                if el.is_displayed():
                    el_text_content = el.text
                    # print(
                    #     f"DEBUG is_reply_structure: ID {tweet_id}, è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆå€™è£œ: '{el_text_content}'"
                    # )
                    if "@" in el_text_content:
                        print(
                            f"ğŸ’¬ is_reply_structure (reply_to_user_text): ID {tweet_id} â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š (True) (ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´: '{el_text_content[:30]}...')"
                        )
                        return True
        except StaleElementReferenceException:
            print(
                f"DEBUG is_reply_structure: StaleElementReferenceException during reply text check {id_display}."
            )
        except Exception as e_indicator:
            print(
                f"âš ï¸ is_reply_structure: è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_indicator).__name__}: {e_indicator}"
            )

        # 5. ãƒœã‚¿ãƒ³ã®æ•°ã«ã‚ˆã‚‹åˆ¤å®š (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        try:
            # å¼•ç”¨RTã‚³ãƒ³ãƒ†ãƒŠå†…éƒ¨ã®ãƒœã‚¿ãƒ³ã¯é™¤å¤–
            action_buttons_group = article.find_elements(
                By.XPATH,
                ".//div[@role='group' and count(.//button[@data-testid]) > 0 and not(ancestor::div[@data-testid='tweetQuote'])]",
            )
            if action_buttons_group:
                visible_group = None
                for group in action_buttons_group:
                    if group.is_displayed():
                        visible_group = group
                        break

                if visible_group:
                    buttons_in_group = visible_group.find_elements(
                        By.XPATH, ".//button[@data-testid]"
                    )
                    visible_buttons_count = sum(
                        1 for btn in buttons_in_group if btn.is_displayed()
                    )

                    # é€šå¸¸ãƒ„ã‚¤ãƒ¼ãƒˆã¯4ã¤(ãƒªãƒ—ãƒ©ã‚¤ã€RTã€ã„ã„ã­ã€ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯/ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹)
                    # ãƒªãƒ—ãƒ©ã‚¤ã¯3ã¤ä»¥ä¸‹(ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯/ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãŒãªã„ã“ã¨ãŒå¤šã„)
                    # ãŸã ã—ã€è‡ªåˆ†ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¸ã®ãƒªãƒ—ãƒ©ã‚¤ãªã©ã€çŠ¶æ³ã«ã‚ˆã£ã¦ãƒœã‚¿ãƒ³æ•°ã¯å¤‰å‹•ã—ã†ã‚‹ã®ã§æ³¨æ„
                    if 0 < visible_buttons_count <= 3:
                        # ã“ã‚ŒãŒæœ¬å½“ã«ãƒªãƒ—ãƒ©ã‚¤ã§ã‚ã‚‹ã‹ã€ã‚‚ã†å°‘ã—ç¢ºè¨¼ãŒã»ã—ã„å ´åˆãŒã‚ã‚‹ã€‚
                        # ä¾‹ãˆã°ã€socialContext ã‚„ è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆ ãŒãªã„å ´åˆã«é™ã‚Šã€ãƒœã‚¿ãƒ³æ•°ã§åˆ¤æ–­ã™ã‚‹ãªã©ã€‚
                        # ã“ã“ã§ã¯ã€ãƒœã‚¿ãƒ³æ•°ãŒå°‘ãªã„å ´åˆã¯ãƒªãƒ—ãƒ©ã‚¤ã®å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã™ã‚‹ã€‚
                        print(
                            f"ğŸ’¬ is_reply_structure (button_count): ID {tweet_id}, Visible Button Count: {visible_buttons_count} (<=3) â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®šã®å¯èƒ½æ€§ (True)"
                        )
                        return True
        except StaleElementReferenceException:
            print(
                f"DEBUG is_reply_structure: StaleElementReferenceException during button count check {id_display}."
            )
        except Exception as e_button_count:
            print(
                f"âš ï¸ is_reply_structure: ãƒœã‚¿ãƒ³æ•°ç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_button_count).__name__}: {e_button_count}"
            )

        # ä¸Šè¨˜ã®ã„ãšã‚Œã«ã‚‚è©²å½“ã—ãªã„å ´åˆã¯ã€åé›†å¯¾è±¡ã®è¦ªæŠ•ç¨¿ã¨ã¿ãªã™
        print(
            f"âœ… is_reply_structure: æ§‹é€ ä¸Šå•é¡Œãªã—ï¼ˆéå¼•ç”¨RTã€éãƒªãƒ—ãƒ©ã‚¤ã€ãƒ¡ãƒ‡ã‚£ã‚¢ãƒã‚§ãƒƒã‚¯æ¸ˆï¼‰â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤å®š (False) {id_display}"
        )
        return False  # False = ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ã§ã¯ãªã„ (åé›†å¯¾è±¡)

    except StaleElementReferenceException:
        print(
            f"âš ï¸ is_reply_structure: StaleElementReferenceExceptionç™ºç”Ÿ {id_display} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆFalseï¼‰ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False
    except Exception as e:
        print(
            f"âš ï¸ is_reply_structure: åˆ¤å®šã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e).__name__}: {e}\n{traceback.format_exc()} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆFalseï¼‰ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False


def has_media_in_html(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    # ç”»åƒåˆ¤å®š
    if soup.find("img", {"src": lambda x: x and "twimg.com/media" in x}):
        return True
    # å‹•ç”»åˆ¤å®š
    if soup.find("div", {"data-testid": "video-player-mini-ui-"}):
        return True
    if soup.find("button", {"aria-label": "å‹•ç”»ã‚’å†ç”Ÿ"}):
        return True
    if soup.find("video"):
        return True
    return False


def extract_tweets(
    driver,
    extract_target,
    max_tweets,
    globally_processed_ids,
    config,
    remaining_needed=None,
):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ç¢ºå®Ÿã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹
    user_profile_url = f"https://twitter.com/{extract_target}"
    print(f"\nâœ¨ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {user_profile_url}")
    driver.get(
        user_profile_url
    )  # ã“ã®è¡Œã‚’è¿½åŠ : æ˜ç¤ºçš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
    time.sleep(3)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ

    print(
        f"DEBUG extract_tweets: globally_processed_ids (type: {type(globally_processed_ids)}, size: {len(globally_processed_ids)}) received. Sample: {list(globally_processed_ids)[:5] if globally_processed_ids else 'empty'}"
    )

    # URLã®å–å¾—ä¸Šé™ã‚’è¨ˆç®—
    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯æ®‹ã‚Šå¿…è¦æŠ•ç¨¿æ•°ã‚’å„ªå…ˆçš„ã«è€ƒæ…®ã™ã‚‹
    url_collection_limit = None

    if remaining_needed is not None and remaining_needed > 0:
        # æ®‹ã‚Šå¿…è¦æŠ•ç¨¿æ•°ãŒ25ä»¥ä¸‹ã®å ´åˆã¯æ®‹ã‚Šå¿…è¦æŠ•ç¨¿æ•°Ã—2ã€ãã‚Œä»¥ä¸Šã®å ´åˆã¯50ã‚’ä¸Šé™ã¨ã™ã‚‹
        url_collection_limit = (
            min(remaining_needed * 2, 50) if remaining_needed <= 25 else 50
        )
        print(
            f"â„¹ï¸ æ®‹ã‚Šå¿…è¦æ•°: {remaining_needed}ä»¶ã€URLå–å¾—ä¸Šé™: {url_collection_limit}ä»¶ã«èª¿æ•´"
        )
    else:
        # æ®‹ã‚Šå¿…è¦æŠ•ç¨¿æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯max_tweetsã‚’ä½¿ç”¨
        url_collection_limit = min(max_tweets * 2, 50) if max_tweets <= 25 else 50
        print(
            f"â„¹ï¸ åˆå›å–å¾—: max_tweets={max_tweets}ä»¶ã€URLå–å¾—ä¸Šé™: {url_collection_limit}ä»¶ã«è¨­å®š"
        )

    tweet_urls = []
    seen_urls_in_current_call = set()

    scroll_count = 0
    max_scrolls = config.get("max_scrolls_extract_tweets", 20)
    pause_threshold = config.get("pause_threshold_extract_tweets", 6)
    scroll_pause_time = config.get("scroll_pause_time", 2.5)
    pause_counter = 0
    consecutive_stale_errors_article_loop = 0
    MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL = 3

    while scroll_count < max_scrolls and len(tweet_urls) < url_collection_limit:
        print(
            f"\nğŸ” ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {scroll_count + 1}/{max_scrolls} å›ç›® (åé›†æ¸ˆã¿: {len(tweet_urls)}/{url_collection_limit})"
        )

        current_articles_in_dom = []
        try:
            current_articles_in_dom = driver.find_elements(
                By.XPATH, "//article[@data-testid='tweet']"
            )
            print(f"ğŸ“„ ç¾åœ¨ã®articleæ•°: {len(current_articles_in_dom)}")
            if not current_articles_in_dom and scroll_count > 0:
                print("âš ï¸ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã€articleè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                time.sleep(5)
                scroll_count += 1
                continue
        except Exception as e_find_articles:
            print(f"âš ï¸ articleè¦ç´ ã®æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_find_articles}")
            time.sleep(5)
            scroll_count += 1
            continue

        new_tweets_found_in_scroll = 0

        for i, article_element_to_process in enumerate(current_articles_in_dom):
            current_tweet_url_for_log = "ä¸æ˜"
            current_tweet_id_for_log = "ä¸æ˜"

            try:
                try:
                    if not article_element_to_process.is_displayed():
                        continue
                except StaleElementReferenceException:
                    print(
                        f"DEBUG extract_tweets: Article {i} is stale at the beginning of the loop. Breaking inner loop for this scroll."
                    )
                    consecutive_stale_errors_article_loop += 1
                    if (
                        consecutive_stale_errors_article_loop
                        >= MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL
                    ):
                        print(
                            f"ğŸš« 1ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å†…ã§Staleã‚¨ãƒ©ãƒ¼ãŒ{MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL}å›é€£ç¶šã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’ä¸­æ–­ã€‚"
                        )
                        break
                    continue

                main_link_elements = article_element_to_process.find_elements(
                    By.XPATH,
                    ".//a[.//time[@datetime] and contains(@href, '/status/') and not(ancestor::div[contains(@style, 'display: none')])]",
                )

                tweet_url = ""
                tweet_id = ""

                if main_link_elements:
                    for link_el in main_link_elements:
                        href_attr = link_el.get_attribute("href")
                        if href_attr and "/status/" in href_attr:
                            match = re.search(
                                r"twitter\.com/([^/]+)/status/(\d+)",
                                href_attr,
                                re.IGNORECASE,
                            ) or re.search(
                                r"x\.com/([^/]+)/status/(\d+)", href_attr, re.IGNORECASE
                            )
                            if match:
                                url_user = match.group(1)
                                potential_id = match.group(2)
                                if url_user.lower() == extract_target.lower():
                                    tweet_url = href_attr
                                    tweet_id = potential_id
                                    break
                    if not tweet_url and main_link_elements:
                        first_href = main_link_elements[0].get_attribute("href")
                        if first_href and "/status/" in first_href:
                            match_fb_id = re.search(r"/status/(\d+)", first_href)
                            if match_fb_id and not tweet_id:
                                pass

                if not tweet_id:
                    continue

                if not tweet_url:
                    tweet_url = f"https://x.com/{extract_target}/status/{tweet_id}"

                current_tweet_url_for_log = tweet_url
                current_tweet_id_for_log = tweet_id

                if tweet_url in seen_urls_in_current_call:
                    continue

                if tweet_id in globally_processed_ids:
                    seen_urls_in_current_call.add(tweet_url)
                    continue

                username_in_url_match = re.search(
                    r"x\.com/([^/]+)/status", tweet_url, re.IGNORECASE
                ) or re.search(r"twitter\.com/([^/]+)/status", tweet_url, re.IGNORECASE)
                if (
                    not username_in_url_match
                    or username_in_url_match.group(1).lower() != extract_target.lower()
                ):
                    seen_urls_in_current_call.add(tweet_url)
                    continue

                text_el = None
                text = ""
                try:
                    text_el = article_element_to_process.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    if text_el:
                        text = normalize_text(text_el.text)
                except NoSuchElementException:
                    pass
                except StaleElementReferenceException:
                    print(
                        f"DEBUG extract_tweets: Stale text element for ID {current_tweet_id_for_log}. Breaking inner loop for this scroll."
                    )
                    consecutive_stale_errors_article_loop += 1
                    break

                # --- ãƒ¡ãƒ‡ã‚£ã‚¢æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ ---
                images_for_check = []
                videos_for_check = []

                quote_container_in_this_article = None
                try:
                    quote_elements_by_testid = article_element_to_process.find_elements(
                        By.XPATH, ".//div[@data-testid='tweetQuote']"
                    )
                    for qc_el in quote_elements_by_testid:
                        if qc_el.is_displayed():
                            quote_container_in_this_article = qc_el
                            # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Found quote container by testid.")
                            break

                    if not quote_container_in_this_article:
                        quote_text_indicator = None
                        qti_elements = article_element_to_process.find_elements(
                            By.XPATH,
                            ".//span[normalize-space(.)='å¼•ç”¨' or normalize-space(.)='Quote'][not(ancestor::div[@data-testid='tweetText' or @data-testid='tweetQuote'])] | "
                            ".//div[normalize-space(.)='å¼•ç”¨' or normalize-space(.)='Quote'][not(ancestor::div[@data-testid='tweetText' or @data-testid='tweetQuote'])]",
                        )
                        for qti_el in qti_elements:
                            if qti_el.is_displayed():
                                try:
                                    closest_article = qti_el.find_element(
                                        By.XPATH,
                                        "ancestor::article[@data-testid='tweet'][1]",
                                    )
                                    if closest_article == article_element_to_process:
                                        quote_text_indicator = qti_el
                                        # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Found quote_text_indicator: '{quote_text_indicator.text}'")
                                        break
                                except (
                                    NoSuchElementException,
                                    StaleElementReferenceException,
                                ):
                                    pass

                        if quote_text_indicator:
                            potential_qlcs = article_element_to_process.find_elements(
                                By.XPATH,
                                ".//div[@role='link' and .//time[@datetime] and not(ancestor::div[@data-testid='tweetText' or @data-testid='tweetQuote'])]",
                            )
                            for qlc_candidate in potential_qlcs:
                                if qlc_candidate.is_displayed():
                                    try:
                                        closest_article_qlc = qlc_candidate.find_element(
                                            By.XPATH,
                                            "ancestor::article[@data-testid='tweet'][1]",
                                        )
                                        if (
                                            closest_article_qlc
                                            != article_element_to_process
                                        ):
                                            continue
                                        position_check = driver.execute_script(
                                            "return arguments[0].compareDocumentPosition(arguments[1])",
                                            quote_text_indicator,
                                            qlc_candidate,
                                        )
                                        if position_check & 4:
                                            quote_container_in_this_article = (
                                                qlc_candidate
                                            )
                                            # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Found qlc structurally following indicator.")
                                            break
                                    except Exception as e_js_exec:
                                        print(
                                            f"DEBUG extract_tweets: JS exec error or stale element for position check (ID {current_tweet_id_for_log}): {e_js_exec}"
                                        )
                except StaleElementReferenceException:
                    print(
                        f"DEBUG extract_tweets: Stale quote container check for ID {current_tweet_id_for_log}. Breaking inner loop."
                    )
                    consecutive_stale_errors_article_loop += 1
                    break
                except Exception as e_qc_find_extract:
                    print(
                        f"DEBUG extract_tweets: Error finding quote container (ID {current_tweet_id_for_log}): {e_qc_find_extract}"
                    )

                # ç”»åƒæŠ½å‡º
                try:
                    # Find all img elements that could be media first
                    all_potential_raw_imgs = article_element_to_process.find_elements(
                        By.XPATH,
                        ".//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                    )
                    # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Found {len(all_potential_raw_imgs)} potential raw img elements.")

                    for img_el in all_potential_raw_imgs:
                        try:
                            visual_container = None
                            try:  # Check if it's within a tweetPhoto container
                                visual_container = img_el.find_element(
                                    By.XPATH,
                                    "ancestor::div[@data-testid='tweetPhoto'][1]",
                                )
                            except NoSuchElementException:
                                try:  # If not, check if it's within a card.layout container
                                    visual_container = img_el.find_element(
                                        By.XPATH,
                                        "ancestor::div[contains(@data-testid, 'card.layout')][1]",
                                    )
                                except NoSuchElementException:
                                    # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Img has no 'tweetPhoto' or 'card.layout' ancestor: {img_el.get_attribute('src')}")
                                    continue

                            if not visual_container.is_displayed():
                                # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Img's visual container not displayed: {img_el.get_attribute('src')}")
                                continue

                            is_inside_quote = False
                            if quote_container_in_this_article:
                                try:
                                    is_inside_quote = driver.execute_script(
                                        "return arguments[0].contains(arguments[1]);",
                                        quote_container_in_this_article,
                                        visual_container,
                                    )
                                except Exception:  # Fallback if JS fails
                                    try:
                                        if quote_container_in_this_article.find_elements(
                                            By.XPATH,
                                            f".//img[@src=\"{img_el.get_attribute('src')}\"]",
                                        ):
                                            is_inside_quote = True
                                    except:
                                        pass  # Ignore StaleElement or other errors in fallback

                            if not is_inside_quote:
                                try:
                                    closest_ancestor_article_for_media = visual_container.find_element(
                                        By.XPATH,
                                        "ancestor::article[@data-testid='tweet'][1]",
                                    )
                                    if (
                                        closest_ancestor_article_for_media
                                        == article_element_to_process
                                    ):
                                        src = img_el.get_attribute("src")
                                        if src and src not in images_for_check:
                                            images_for_check.append(src)
                                            # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Added image to check: {src}")
                                except (
                                    NoSuchElementException,
                                    StaleElementReferenceException,
                                ):
                                    continue
                        except StaleElementReferenceException:
                            continue
                        except Exception as e_img_inner:
                            print(
                                f"DEBUG extract_tweets: Error processing one image for {current_tweet_id_for_log}: {e_img_inner}"
                            )
                            continue

                except StaleElementReferenceException:
                    print(
                        f"DEBUG extract_tweets: Stale image elements check for ID {current_tweet_id_for_log}. Breaking."
                    )
                    consecutive_stale_errors_article_loop += 1
                    break
                except Exception as e_img_extract_outer_loop:
                    print(
                        f"DEBUG extract_tweets: Error during image extraction for ID {current_tweet_id_for_log}: {e_img_extract_outer_loop}"
                    )

                # å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼æŠ½å‡º
                try:
                    potential_video_player_containers = article_element_to_process.find_elements(
                        By.XPATH,
                        ".//div[(@data-testid='videoPlayer' or @data-testid='videoComponent' or @data-testid='communitynotesVideo' or contains(@data-testid, 'playerInstance'))]",
                    )
                    for player_container_el in potential_video_player_containers:
                        try:
                            if not player_container_el.is_displayed():
                                continue

                            video_elements_in_container = (
                                player_container_el.find_elements(
                                    By.XPATH, ".//video[@poster]"
                                )
                            )
                            for video_el in video_elements_in_container:
                                is_inside_quote_video = False
                                if quote_container_in_this_article:
                                    try:
                                        is_inside_quote_video = driver.execute_script(
                                            "return arguments[0].contains(arguments[1]);",
                                            quote_container_in_this_article,
                                            player_container_el,
                                        )
                                    except Exception:  # Fallback
                                        try:
                                            if quote_container_in_this_article.find_elements(
                                                By.XPATH,
                                                f".//video[@poster=\"{video_el.get_attribute('poster')}\"]",
                                            ):
                                                is_inside_quote_video = True
                                        except:
                                            pass

                                if not is_inside_quote_video:
                                    try:
                                        closest_ancestor_article_for_media = player_container_el.find_element(
                                            By.XPATH,
                                            "ancestor::article[@data-testid='tweet'][1]",
                                        )
                                        if (
                                            closest_ancestor_article_for_media
                                            == article_element_to_process
                                        ):
                                            poster = video_el.get_attribute("poster")
                                            if poster and not any(
                                                vp_check.endswith(
                                                    poster.split("/")[-1].split("?")[0]
                                                )
                                                for vp_check in videos_for_check
                                            ):
                                                videos_for_check.append(poster)
                                                # print(f"DEBUG extract_tweets: ID {current_tweet_id_for_log}, Added video poster: {poster}")
                                    except (
                                        NoSuchElementException,
                                        StaleElementReferenceException,
                                    ):
                                        continue
                        except StaleElementReferenceException:
                            continue
                        except Exception as e_vid_inner:
                            print(
                                f"DEBUG extract_tweets: Error processing one video container for {current_tweet_id_for_log}: {e_vid_inner}"
                            )
                            continue

                except StaleElementReferenceException:
                    print(
                        f"DEBUG extract_tweets: Stale video elements check for ID {current_tweet_id_for_log}. Breaking."
                    )
                    consecutive_stale_errors_article_loop += 1
                    break
                except Exception as e_vid_extract_outer_loop:
                    print(
                        f"DEBUG extract_tweets: Error during video extraction for ID {current_tweet_id_for_log}: {e_vid_extract_outer_loop}"
                    )

                # print(f"DEBUG extract_tweets: Final media for ID {current_tweet_id_for_log} -> Images: {len(images_for_check)}, Videos: {len(videos_for_check)}")

                if is_ad_post(text):
                    seen_urls_in_current_call.add(tweet_url)
                    continue

                if is_reply_structure(
                    article_element_to_process,
                    tweet_id=current_tweet_id_for_log,
                    text=text,
                    image_urls=images_for_check,
                    video_poster_urls=videos_for_check,
                ):
                    seen_urls_in_current_call.add(tweet_url)
                    continue

                tweet_urls.append({"url": tweet_url, "id": current_tweet_id_for_log})
                seen_urls_in_current_call.add(tweet_url)
                new_tweets_found_in_scroll += 1
                consecutive_stale_errors_article_loop = 0

                print(f"âœ… åé›†å€™è£œã«è¿½åŠ : {tweet_url} ({len(tweet_urls)}ä»¶ç›®)")
                if len(tweet_urls) >= url_collection_limit:
                    break

            except StaleElementReferenceException:
                print(
                    f"âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿ (Article {i}, ID: {current_tweet_id_for_log}, URL: {current_tweet_url_for_log})ã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§ã®å‡¦ç†ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚"
                )
                consecutive_stale_errors_article_loop += 1
                if (
                    consecutive_stale_errors_article_loop
                    >= MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL
                ):
                    print(
                        f"ğŸš« 1ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å†…ã§Staleã‚¨ãƒ©ãƒ¼ãŒ{MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL}å›é€£ç¶šã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’ä¸­æ–­ã€‚"
                    )
                break
            except Exception as e_article_loop_main:
                print(
                    f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºãƒ«ãƒ¼ãƒ—å†…ã‚¨ãƒ©ãƒ¼ (Article {i}, ID: {current_tweet_id_for_log}, URL: {current_tweet_url_for_log}): {type(e_article_loop_main).__name__} - {e_article_loop_main}"
                )
                continue

        if (
            consecutive_stale_errors_article_loop
            >= MAX_CONSECUTIVE_STALE_ERRORS_IN_SCROLL
        ):
            print(
                f"DEBUG extract_tweets: Exiting inner article loop for scroll {scroll_count + 1} due to repeated stale errors."
            )
            consecutive_stale_errors_article_loop = 0

        if len(tweet_urls) >= url_collection_limit:
            print(
                f"ğŸ¯ åé›†å€™è£œæ•°ãŒä¸Šé™ ({url_collection_limit}) ã«é”ã—ãŸãŸã‚ã€URLåé›†ã‚’çµ‚äº†ã€‚"
            )
            break

        if new_tweets_found_in_scroll == 0 and scroll_count > 0:
            pause_counter += 1
            print(
                f"ğŸ§Š ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§æ–°è¦æŠ•ç¨¿ãªã— â†’ pause_counter={pause_counter}/{pause_threshold}"
            )
            # å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã«æˆ»ã™: ä¸€å®šæ•°ä»¥ä¸Šã®æŠ•ç¨¿URLãŒé›†ã¾ã£ã¦ã„ã‚‹å ´åˆã®ã¿ä¸­æ–­
            if pause_counter >= pause_threshold and len(tweet_urls) >= (
                url_collection_limit / 2
            ):
                print("ğŸ›‘ æ–°ã—ã„æŠ•ç¨¿ãŒé€£ç¶šã—ã¦æ¤œå‡ºã•ã‚Œãªã„ãŸã‚URLåé›†ã‚’ä¸­æ–­")
                break
        else:
            pause_counter = 0

        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause_time)
        except Exception as e_scroll_exec:
            print(f"âš ï¸ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_scroll_exec}")
            break

        scroll_count += 1

    print(f"\nğŸ“ˆ åé›†å€™è£œã®URLå–å¾—å®Œäº† â†’ åˆè¨ˆ: {len(tweet_urls)} ä»¶")
    tweet_urls.sort(key=lambda x: int(x.get("id", 0)), reverse=True)

    # URLã‚’åé›†ã—ãŸå¾Œã€IDã‚’æŠ½å‡ºã—ã¦æ—¢ã«å‡¦ç†æ¸ˆã¿ã®IDã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    collected_urls_filtered = []
    for url in tweet_urls:
        tweet_id = re.search(r"/status/(\d+)", url["url"])
        if tweet_id and tweet_id.group(1) not in globally_processed_ids:
            collected_urls_filtered.append(url)
        else:
            print(
                f"ğŸš« é‡è¤‡IDæ¤œå‡º: {tweet_id.group(1) if tweet_id else 'unknown'} - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
            )

    return collected_urls_filtered  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®URLãƒªã‚¹ãƒˆã‚’è¿”ã™


def already_registered(tweet_id, database_id_to_check, notion_client_instance):
    if not tweet_id or not str(tweet_id).isdigit() or not database_id_to_check:
        print(
            f"â„¹ï¸ already_registered: ç„¡åŠ¹ãªå¼•æ•° (tweet_id: {tweet_id}, db_id: {database_id_to_check})"
        )
        return False  # ä¸æ­£ãªIDã®å ´åˆã¯æœªç™»éŒ²æ‰±ã„ã®æ–¹ãŒå®‰å…¨ã‹ã€ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ã‹

    query_filter = {"property": "æŠ•ç¨¿ID", "rich_text": {"equals": str(tweet_id)}}

    try:
        # print(f"ğŸ” DBç™»éŒ²ç¢ºèª: Tweet ID {tweet_id} in DB {database_id_to_check}")
        result = notion_client_instance.databases.query(
            database_id=database_id_to_check, filter=query_filter
        )
        num_results = len(result.get("results", []))
        # if num_results > 0:
        #     print(f"Found {num_results} existing entries for Tweet ID {tweet_id} in DB {database_id_to_check}")
        return num_results > 0
    except Exception as e:
        print(
            f"âš ï¸ Notionã‚¯ã‚¨ãƒªå¤±æ•— (DBç™»éŒ²ç¢ºèª): DB {database_id_to_check}, ID {tweet_id}"
        )
        print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__} - {e}")
        return False  # ã‚¯ã‚¨ãƒªå¤±æ•—æ™‚ã¯æœªç™»éŒ²ã¨ã—ã¦æ‰±ã†ï¼ˆå†è©¦è¡Œã®æ©Ÿä¼šã‚’ä¸ãˆã‚‹ï¼‰


def ocr_and_remove_image(image_path, label=None):
    """
    ç”»åƒãƒ‘ã‚¹ã‚’å—ã‘å–ã‚ŠOCRã—ã€ä½¿ç”¨å¾Œã«å‰Šé™¤ã™ã‚‹ã€‚
    labelãŒã‚ã‚Œã°çµæœã®å…ˆé ­ã«ä»˜ä¸ã€‚
    """
    result = ""
    try:
        ocr_result = ocr_image(image_path)
        if ocr_result:
            cleaned = clean_ocr_text(ocr_result)
            result = f"[{label}]\n{cleaned}" if label else cleaned
    except Exception as e:
        print(f"âš ï¸ OCRå¤±æ•—: {e}")
    finally:
        try:
            os.remove(image_path)
            print(f"ğŸ—‘ï¸ ç”»åƒå‰Šé™¤: {image_path}")
        except Exception as e:
            print(f"âš ï¸ ç”»åƒå‰Šé™¤å¤±æ•—: {e}")
    return result


def clean_ocr_text(text):
    # é™¤å¤–ã—ãŸã„æ–‡è¨€ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã“ã“ã«è¿½åŠ 
    EXCLUDE_PATTERNS = [
        "æœè³ªå•ã‚’ã€Œã„ã„ã­!ã€ ã™ã‚‹",
        "ã“ã®æŠ•ç¨¿ã‚’ã„ã„ã­ï¼",
        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    ]
    lines = text.splitlines()
    cleaned = [
        line for line in lines if not any(pat in line for pat in EXCLUDE_PATTERNS)
    ]
    return "\n".join(cleaned)


def get_all_registered_ids_from_db(
    database_id, notion_client, db_type_for_log="unknown"
):
    """æŒ‡å®šã•ã‚ŒãŸNotionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨ã¦ã®ã€ŒæŠ•ç¨¿IDã€ã‚’å–å¾—ã™ã‚‹ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    if not database_id:
        print(
            f"âš ï¸ get_all_registered_ids_from_db: database_id (type: {db_type_for_log}) ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        )
        return set()

    all_ids = set()
    has_more = True
    start_cursor = None
    page_count = 0
    print(f"ğŸ”„ DB ({db_type_for_log}) ã‹ã‚‰æ—¢å­˜ã®æŠ•ç¨¿IDã‚’å–å¾—é–‹å§‹: {database_id}")

    while has_more:
        try:
            page_count += 1
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page_count} ã‚’å–å¾—ä¸­...")
            response = notion_client.databases.query(
                database_id=database_id,
                filter={
                    "property": "æŠ•ç¨¿ID",  # "å‡¦ç†æ¸ˆã¿æŠ•ç¨¿IDãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹" ã®å ´åˆã¯ã“ã‚ŒãŒ Title ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
                    "rich_text": {"is_not_empty": True},
                },
                page_size=100,
                start_cursor=start_cursor,
            )
            results = response.get("results", [])
            for item in results:
                properties = item.get("properties", {})
                # "å‡¦ç†æ¸ˆã¿æŠ•ç¨¿IDãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹" ã®å ´åˆã€Titleãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒæŠ•ç¨¿IDã‚’ä¿æŒã™ã‚‹ã¨æƒ³å®š
                post_id_prop_key = "æŠ•ç¨¿ID"  # ã“ã‚Œã¯Titleãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®åå‰ã‚’æŒ‡ã™

                post_id_prop = properties.get(post_id_prop_key, {})

                # Titleãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®å ´åˆã®å‡¦ç†
                if post_id_prop.get("type") == "title":
                    title_array = post_id_prop.get("title", [])
                    if title_array:
                        tweet_id_val = title_array[0].get("plain_text")
                        if tweet_id_val and tweet_id_val.isdigit():
                            all_ids.add(tweet_id_val)
                # å¾“æ¥ã® rich_text ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®å ´åˆã®å‡¦ç† (å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™)
                elif post_id_prop.get("type") == "rich_text":
                    rich_text_array = post_id_prop.get("rich_text", [])
                    if rich_text_array:
                        tweet_id_val = rich_text_array[0].get("plain_text")
                        if tweet_id_val and tweet_id_val.isdigit():
                            all_ids.add(tweet_id_val)
                else:
                    # "æŠ•ç¨¿ID" ã¨ã„ã†åå‰ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒ title ã§ã‚‚ rich_text ã§ã‚‚ãªã„å ´åˆ
                    # ã¾ãŸã¯ã€ã‚­ãƒ¼ãŒç•°ãªã‚‹å ´åˆã¯ã€ãƒ­ã‚°ã‚’å‡ºã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                    # print(f"DEBUG: DB {database_id}, item {item.get('id')}, 'æŠ•ç¨¿ID' property is not title or rich_text, or key is different. Type: {post_id_prop.get('type')}")
                    pass

            has_more = response.get("has_more", False)
            start_cursor = response.get("next_cursor")
            if has_more:
                print(f"  ...ã•ã‚‰ã«ãƒšãƒ¼ã‚¸ã‚ã‚Š (å–å¾—æ¸ˆã¿IDæ•°: {len(all_ids)})")
                time.sleep(0.5)
        except Exception as e:
            print(
                f"âŒ DB ({db_type_for_log}) ã‹ã‚‰ã®IDå–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ (DB: {database_id}, Page: {page_count}): {e}"
            )
            print(
                f"   ç¾åœ¨ã®å–å¾—æ¸ˆã¿IDæ•°: {len(all_ids)}ã€‚ã“ã®DBã®IDå–å¾—ã‚’ä¸­æ–­ã—ã¾ã™ã€‚"
            )
            break
    print(
        f"âœ… DB ({db_type_for_log}) ã‹ã‚‰ã®IDå–å¾—å®Œäº†: {database_id}, åˆè¨ˆ {len(all_ids)} ä»¶"
    )
    return all_ids


def add_to_processed_ids_db(
    notion_client,
    processed_db_id,
    post_id,
    processing_type,
    parent_post_id=None,
    url=None,
):
    if not processed_db_id:
        print("âš ï¸ å‡¦ç†æ¸ˆã¿ID DBã®IDãŒæœªè¨­å®šã®ãŸã‚ã€ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return False
    if not post_id:
        print("âš ï¸ å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ç™»éŒ²ã™ã‚‹æŠ•ç¨¿IDãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return False

    props = {
        "æŠ•ç¨¿ID": {"title": [{"type": "text", "text": {"content": str(post_id)}}]},
        "å‡¦ç†ã‚¿ã‚¤ãƒ—": {"select": {"name": processing_type}},
        "å‡¦ç†æ—¥æ™‚": {"date": {"start": datetime.now().isoformat()}},
    }
    if parent_post_id:
        props["è¦ªæŠ•ç¨¿ID"] = {
            "rich_text": [{"type": "text", "text": {"content": str(parent_post_id)}}]
        }
    if url:
        props["URL"] = {"url": url}

    try:
        notion_client.pages.create(
            parent={"database_id": processed_db_id}, properties=props
        )
        print(
            f"âœ… å‡¦ç†æ¸ˆã¿ID DBç™»éŒ²: ID={post_id}, ã‚¿ã‚¤ãƒ—={processing_type}, è¦ªID={parent_post_id if parent_post_id else 'N/A'}, URL={url if url else 'N/A'}"
        )
        return True
    except Exception as e:
        print(f"âŒ å‡¦ç†æ¸ˆã¿ID DBç™»éŒ²å¤±æ•—: ID={post_id}, ã‚¿ã‚¤ãƒ—={processing_type} - {e}")
        return False


def upload_to_notion(
    tweet, config, notion_client, registered_ids_map, processed_post_ids_db_id
):
    print(f"ğŸ“¤ Notionç™»éŒ²æº–å‚™é–‹å§‹: {tweet.get('id', 'IDä¸æ˜')}")

    # 1. GPTå‡¦ç†ç”¨ã®å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    ocr_texts_for_gpt = []
    temp_image_paths_for_gpt_ocr = []  # GPT OCRå‡¦ç†å¾Œã«å‰Šé™¤ã™ã‚‹ä¸€æ™‚ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

    # tweetè¾æ›¸å†…ã®ç”»åƒURLã‹ã‚‰ä¸€æ™‚ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR
    for idx, img_url in enumerate(tweet.get("images", [])):
        img_filename = f"temp_ocr_image_for_gpt_{tweet.get('id', 'unknown')}_{idx}.jpg"
        temp_ocr_image_dir = "temp_ocr_images"  # ä¸€æ™‚ç”»åƒä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if not os.path.exists(temp_ocr_image_dir):
            os.makedirs(temp_ocr_image_dir, exist_ok=True)
        img_path_temp = os.path.join(temp_ocr_image_dir, img_filename)
        temp_image_paths_for_gpt_ocr.append(img_path_temp)  # å‰Šé™¤ãƒªã‚¹ãƒˆã«è¿½åŠ 

        try:
            print(f"Downloading image for GPT OCR: {img_url} to {img_path_temp}")
            resp = requests.get(
                img_url, stream=True, timeout=20
            )  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—é•·ã‚ã«
            resp.raise_for_status()
            with open(img_path_temp, "wb") as f:
                for chunk in resp.iter_content(8192):  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª¿æ•´
                    f.write(chunk)
            raw_ocr_text = ocr_image(
                img_path_temp
            )  # ocr_image ã¯å†…éƒ¨ã§ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’æŒã¤æƒ³å®š
            if raw_ocr_text and raw_ocr_text.strip() and raw_ocr_text != "[OCRã‚¨ãƒ©ãƒ¼]":
                ocr_texts_for_gpt.append(f"[ç”»åƒ{idx+1}]\n{raw_ocr_text.strip()}")
            else:
                print(
                    f"â„¹ï¸ ç”»åƒ{idx+1}ã®OCRçµæœãŒç©ºã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (URL: {img_url})"
                )
        except requests.exceptions.RequestException as e_req:
            print(f"âš ï¸ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— (GPTç”¨): {img_url}, {e_req}")
        except Exception as e_ocr_prep:
            print(f"âš ï¸ GPTç”¨OCRæº–å‚™ã‚¨ãƒ©ãƒ¼ (ç”»åƒ{idx+1}): {e_ocr_prep}")

    # tweetè¾æ›¸å†…ã®å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰OCR (ã“ã‚Œã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®ã¯ãš)
    poster_paths_from_tweet = tweet.get("video_posters", [])
    if isinstance(poster_paths_from_tweet, str):  # å˜ä¸€ãƒ‘ã‚¹ã®å ´åˆã‚‚ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†
        poster_paths_from_tweet = [poster_paths_from_tweet]

    for idx, poster_path in enumerate(poster_paths_from_tweet):
        if os.path.exists(poster_path):  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
            print(f"Processing video poster for GPT OCR: {poster_path}")
            raw_ocr_text = ocr_image(poster_path)
            if raw_ocr_text and raw_ocr_text.strip() and raw_ocr_text != "[OCRã‚¨ãƒ©ãƒ¼]":
                ocr_texts_for_gpt.append(
                    f"[å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«{idx+1}]\n{raw_ocr_text.strip()}"
                )
            else:
                print(
                    f"â„¹ï¸ å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«{idx+1}ã®OCRçµæœãŒç©ºã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (Path: {poster_path})"
                )
            # å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼ã¯ extract_thread_from_detail_page ã§ä¸€æ™‚ä¿å­˜ã•ã‚Œã€
            # ãã®é–¢æ•°ã¾ãŸã¯å‘¼ã³å‡ºã—å…ƒã§å‰Šé™¤ã•ã‚Œã‚‹æƒ³å®šãªã®ã§ã€ã“ã“ã§ã¯å‰Šé™¤ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ãªã„
        else:
            print(f"âš ï¸ GPTç”¨å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹è¦‹ã¤ã‹ã‚‰ãš: {poster_path}")

    combined_ocr_for_gpt = "\n\n".join(ocr_texts_for_gpt).strip()
    if not combined_ocr_for_gpt:
        print("â„¹ï¸ GPTç”¨ã®OCRå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    data_for_gpt_file = {
        "post_text": tweet.get("text", ""),
        "ocr_text": combined_ocr_for_gpt,
    }
    gpt_input_filename = "tweet_for_gpt.json"
    try:
        with open(gpt_input_filename, "w", encoding="utf-8") as f_out_gpt:
            json.dump(data_for_gpt_file, f_out_gpt, ensure_ascii=False, indent=2)
        print(
            f"ğŸ“ GPTç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ {gpt_input_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ (ID: {tweet.get('id', 'N/A')})"
        )
    except Exception as e_save_json:
        print(f"âŒ GPTç”¨ãƒ‡ãƒ¼ã‚¿ {gpt_input_filename} ã®ä¿å­˜ã«å¤±æ•—: {e_save_json}")
        # ä¸€æ™‚ç”»åƒãŒã‚ã‚Œã°å‰Šé™¤
        for temp_path in temp_image_paths_for_gpt_ocr:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        return "FAILED"

    gpt_output_filename = "gpt_output.json"
    gpt_runner_script_path = (
        "gpt_prompt_runner.py"  # config.json ã‹ã‚‰èª­ã¿è¾¼ã‚€ã‚ˆã†ã«ã—ã¦ã‚‚è‰¯ã„
    )
    if os.path.exists(gpt_output_filename):
        try:
            os.remove(gpt_output_filename)
        except OSError as e:
            print(f"âš ï¸ æ—¢å­˜ã® {gpt_output_filename} ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    try:
        print(f"ğŸš€ gpt_prompt_runner.py ã‚’å®Ÿè¡Œã—ã¾ã™...")
        python_executable = shutil.which("python3") or shutil.which(
            "python"
        )  # ç’°å¢ƒã«åˆã‚ã›ã¦
        if not python_executable:
            print("âŒ Pythonå®Ÿè¡Œå¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            for temp_path in temp_image_paths_for_gpt_ocr:
                if os.path.exists(temp_path):
                    try:
                        os.remove(temp_path)
                    except:
                        pass
            return "FAILED"

        process_result = subprocess.run(
            [python_executable, gpt_runner_script_path],
            check=True,  # ã‚¨ãƒ©ãƒ¼æ™‚ã« CalledProcessError ã‚’ç™ºç”Ÿã•ã›ã‚‹
            capture_output=True,
            text=True,
            encoding="utf-8",  # æ˜ç¤ºçš„ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æŒ‡å®š
            timeout=300,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š (ç§’)
        )
        print(f"âœ… gpt_prompt_runner.py å®Ÿè¡Œå®Œäº†ã€‚")
        if process_result.stdout:
            print(f"   Stdout:\n{process_result.stdout.strip()}")
        if process_result.stderr:  # æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚‚ç¢ºèª
            print(f"   Stderr:\n{process_result.stderr.strip()}")

    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {gpt_runner_script_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for temp_path in temp_image_paths_for_gpt_ocr:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        return "FAILED"
    except subprocess.CalledProcessError as e:
        print(
            f"âŒ gpt_prompt_runner.py ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {e.returncode}):"
        )
        if e.stdout:
            print(f"   Stdout: {e.stdout.strip()}")
        if e.stderr:
            print(f"   Stderr: {e.stderr.strip()}")
        for temp_path in temp_image_paths_for_gpt_ocr:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        return "FAILED"
    except subprocess.TimeoutExpired:
        print(f"âŒ gpt_prompt_runner.py ã®å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")
        for temp_path in temp_image_paths_for_gpt_ocr:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        return "FAILED"
    except Exception as e_subproc:  # ãã®ä»–ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
        print(f"âŒ gpt_prompt_runner.py ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e_subproc}")
        for temp_path in temp_image_paths_for_gpt_ocr:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        return "FAILED"
    finally:
        # GPT OCRç”¨ã®ä¸€æ™‚ç”»åƒã‚’å‰Šé™¤
        for path_to_delete in temp_image_paths_for_gpt_ocr:
            if os.path.exists(path_to_delete):
                try:
                    os.remove(path_to_delete)
                except Exception as e_del_temp:
                    print(f"âš ï¸ GPTç”¨ä¸€æ™‚ç”»åƒå‰Šé™¤å¤±æ•—: {path_to_delete}, {e_del_temp}")
        # ä¸€æ™‚ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç©ºãªã‚‰å‰Šé™¤
        if (
            "temp_ocr_image_dir" in locals()
            and os.path.exists(temp_ocr_image_dir)
            and not os.listdir(temp_ocr_image_dir)
        ):
            try:
                os.rmdir(temp_ocr_image_dir)
            except OSError:  # ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ´ã‚“ã§ã„ã‚‹å ´åˆãªã©
                pass

    # 2. GPTã®å‡ºåŠ›ã‚’èª­ã¿è¾¼ã¿ã€Notionç™»éŒ²ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ä½œæˆ
    gpt_classification = "ä¸æ˜"
    gpt_formatted_ocr = (
        combined_ocr_for_gpt  # GPTãŒæ•´å½¢ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã•ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    )

    if not os.path.exists(gpt_output_filename):
        print(
            f"âš ï¸ {gpt_output_filename} ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚GPTå‡¦ç†ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        )
    else:
        try:
            with open(gpt_output_filename, "r", encoding="utf-8") as f_gpt_res:
                gpt_result = json.load(f_gpt_res)
                gpt_classification = gpt_result.get("classification", "ä¸æ˜")
                gpt_formatted_ocr_from_gpt = gpt_result.get("formatted_text")
                if gpt_formatted_ocr_from_gpt is not None:  # Noneã§ãªã„å ´åˆã®ã¿ä¸Šæ›¸ã
                    gpt_formatted_ocr = gpt_formatted_ocr_from_gpt
                print(
                    f"ğŸ“Š GPTåˆ¤å®šçµæœ: åˆ†é¡='{gpt_classification}', æ•´å½¢å¾ŒOCRæä¾›ã‚ã‚Š='{gpt_formatted_ocr_from_gpt is not None}'"
                )
        except json.JSONDecodeError:
            print(
                f"âš ï¸ {gpt_output_filename} ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚æ•´å½¢å‰OCRã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
            )
        except Exception as e_load_gpt_out:
            print(
                f"âš ï¸ {gpt_output_filename} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_load_gpt_out}ã€‚æ•´å½¢å‰OCRã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
            )

    target_database_id = None
    db_key_for_ids_map = None  # registered_ids_map ã®ã©ã®ã‚­ãƒ¼ã«å¯¾å¿œã™ã‚‹ã‹
    if gpt_classification == "è³ªå•å›ç­”":
        target_database_id = config.get("database_id_question")
        db_key_for_ids_map = "question"  # mainé–¢æ•°ã§ã®ã‚­ãƒ¼ã¨åˆã‚ã›ã‚‹
    elif gpt_classification == "æ¡ˆä»¶æŠ•ç¨¿":
        target_database_id = config.get("database_id_project")
        db_key_for_ids_map = "project"  # mainé–¢æ•°ã§ã®ã‚­ãƒ¼ã¨åˆã‚ã›ã‚‹
    else:  # "ã‚¹ãƒ«ãƒ¼ãƒ‡ãƒ¼ã‚¿" ã‚„ "ä¸æ˜" ãªã©ã€ç™»éŒ²å¯¾è±¡å¤–ã®åˆ†é¡
        print(
            f"â„¹ï¸ GPTåˆ†é¡ '{gpt_classification}' ã¯ç™»éŒ²å¯¾è±¡å¤–ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚Tweet ID: {tweet.get('id')}"
        )
        # å‡¦ç†æ¸ˆã¿DBã¸ã®ç™»éŒ²ã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆåˆ†é¡ã§é™¤å¤–ã•ã‚ŒãŸãŸã‚ï¼‰
        return "SKIPPED_CLASSIFICATION"

    if (
        not target_database_id
    ):  # "è³ªå•å›ç­”" or "æ¡ˆä»¶æŠ•ç¨¿" ã«åˆ†é¡ã•ã‚ŒãŸãŒã€configã«DB IDãŒãªã‹ã£ãŸå ´åˆ
        print(
            f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: åˆ†é¡ '{gpt_classification}' ã«å¯¾å¿œã™ã‚‹DB IDãŒconfig.jsonã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Tweet ID: {tweet.get('id')}"
        )
        return "FAILED_CONFIG"

    current_tweet_id_str = str(tweet.get("id"))

    # ç™»éŒ²æ¸ˆã¿ãƒã‚§ãƒƒã‚¯:
    # ã¾ãšã€å¯¾è±¡ã®DB (è³ªå•DB or æ¡ˆä»¶DB) ã«æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
    # registered_ids_map["question"] ã‚„ registered_ids_map["project"] ã‚’å‚ç…§
    if db_key_for_ids_map and current_tweet_id_str in registered_ids_map.get(
        db_key_for_ids_map, set()
    ):
        print(
            f"ğŸš« ã‚¹ã‚­ãƒƒãƒ— (å¯¾è±¡DB '{db_key_for_ids_map}' ã§ç™»éŒ²æ¸ˆç¢ºèª): {current_tweet_id_str}"
        )
        return "SKIPPED_REGISTERED"

    # æ¬¡ã«ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå‡¦ç†æ¸ˆã¿IDã‚»ãƒƒãƒˆ (å…¨DBã®IDã‚’çµ±åˆã—ãŸã‚‚ã®) ã§ã‚‚ç¢ºèª
    # ã“ã‚Œã«ã‚ˆã‚Šã€ä¾‹ãˆã°éå»ã«æ¡ˆä»¶DBã«ç™»éŒ²ã•ã‚ŒãŸã‚‚ã®ãŒã€ä»Šå›è³ªå•DBã®å€™è£œã«ãªã£ãŸå ´åˆãªã©ã‚’é˜²ã
    if current_tweet_id_str in registered_ids_map.get("all_processed", set()):
        print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ— (ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿IDãƒªã‚¹ãƒˆã§ç¢ºèª): {current_tweet_id_str}")
        # ã“ã®å ´åˆã‚‚ã€æ—¢ã«ã©ã“ã‹ã§å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€å‡¦ç†æ¸ˆã¿DBã¸ã®å†ç™»éŒ²ã¯ä¸è¦
        return "SKIPPED_REGISTERED"

    # Notionãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®æº–å‚™
    tweet_id_str = str(tweet.get("id", ""))  # å¿µã®ãŸã‚å†å–å¾—ãƒ»æ–‡å­—åˆ—åŒ–
    props = {
        "æŠ•ç¨¿ID": {
            "rich_text": (
                [{"type": "text", "text": {"content": tweet_id_str}}]
                if tweet_id_str
                else None
            )
        },
        "æœ¬æ–‡": {
            "rich_text": [{"type": "text", "text": {"content": tweet.get("text", "")}}]
        },
        "URL": {"url": tweet.get("url")},
        "æŠ•ç¨¿æ—¥æ™‚": {
            "date": {"start": tweet.get("date")} if tweet.get("date") else None
        },
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": {"select": {"name": "æœªå›ç­”"}},  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°": {
            "number": (
                int(tweet["impressions"])
                if tweet.get("impressions") is not None
                and str(tweet.get("impressions")).isdigit()
                else None
            )
        },
        "ãƒªãƒã‚¹ãƒˆæ•°": {
            "number": (
                int(tweet["retweets"])
                if tweet.get("retweets") is not None
                and str(tweet.get("retweets")).isdigit()
                else 0
            )
        },
        "ã„ã„ã­æ•°": {
            "number": (
                int(tweet["likes"])
                if tweet.get("likes") is not None and str(tweet.get("likes")).isdigit()
                else 0
            )
        },
        "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°": {
            "number": (
                int(tweet["bookmarks"])
                if tweet.get("bookmarks") is not None
                and str(tweet.get("bookmarks")).isdigit()
                else 0
            )
        },
        "ãƒªãƒ—ãƒ©ã‚¤æ•°": {
            "number": (
                int(tweet["replies"])
                if tweet.get("replies") is not None
                and str(tweet.get("replies")).isdigit()
                else 0
            )
        },
        "æ–‡å­—èµ·ã“ã—": {
            "rich_text": [
                {
                    "type": "text",
                    "text": {"content": gpt_formatted_ocr if gpt_formatted_ocr else ""},
                }
            ]
        },
    }
    # None ã®å€¤ã‚’æŒã¤ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é™¤å¤– (Notion APIãŒã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ã“ã¨ãŒã‚ã‚‹ãŸã‚)
    props = {k: v for k, v in props.items() if v is not None}
    # "æŠ•ç¨¿æ—¥æ™‚" ãŒNoneã®å ´åˆã€dateãƒ—ãƒ­ãƒ‘ãƒ†ã‚£è‡ªä½“ã‚’å‰Šé™¤
    if props.get("æŠ•ç¨¿æ—¥æ™‚") and not props["æŠ•ç¨¿æ—¥æ™‚"]["date"]["start"]:
        props.pop("æŠ•ç¨¿æ—¥æ™‚")

    children_blocks = []  # å¿…è¦ã§ã‚ã‚Œã°ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ 

    try:
        print(
            f"ğŸ“¤ Notionã¸ç™»éŒ²å®Ÿè¡Œä¸­... DB ID: {target_database_id}, Tweet ID: {tweet_id_str}"
        )
        new_page = notion_client.pages.create(
            parent={"database_id": target_database_id},
            properties=props,
            children=children_blocks,
        )
        print(f"âœ… Notionç™»éŒ²å®Œäº† (DB: {target_database_id}): {tweet.get('url')}")

        # ç™»éŒ²æˆåŠŸã—ãŸã‚‰ã€ãƒ¡ãƒ¢ãƒªä¸Šã®IDã‚»ãƒƒãƒˆã‚‚æ›´æ–°
        if db_key_for_ids_map and current_tweet_id_str:
            registered_ids_map.setdefault(db_key_for_ids_map, set()).add(
                current_tweet_id_str
            )
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿IDã‚»ãƒƒãƒˆã‚‚æ›´æ–°
        registered_ids_map.setdefault("all_processed", set()).add(current_tweet_id_str)

        # ã€Œå‡¦ç†æ¸ˆã¿æŠ•ç¨¿IDãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ã«ã‚‚è¨˜éŒ²
        # è¦ªæŠ•ç¨¿ã®å ´åˆ
        add_to_processed_ids_db(
            notion_client,
            processed_post_ids_db_id,
            current_tweet_id_str,
            "è¦ªæŠ•ç¨¿",
            url=tweet.get("url"),
        )

        # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤IDã‚‚å‡¦ç†æ¸ˆã¿DBã«è¨˜éŒ²
        merged_reply_ids_list = tweet.get("merged_reply_ids", [])
        if isinstance(
            merged_reply_ids_list, str
        ):  # æ–‡å­—åˆ—ã§æ¸¡ã•ã‚ŒãŸå ´åˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãªã©ï¼‰ã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
            merged_reply_ids_list = [
                rid.strip()
                for rid in merged_reply_ids_list.split(",")
                if rid.strip() and rid.strip().isdigit()
            ]

        for reply_id in merged_reply_ids_list:
            if reply_id and str(reply_id).isdigit():  # æ•°å€¤ã§ã‚ã‚‹ã“ã¨ã‚‚ç¢ºèª
                # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ãŒæ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã‚»ãƒƒãƒˆã«ãªã‘ã‚Œã°è¿½åŠ 
                if str(reply_id) not in registered_ids_map.get("all_processed", set()):
                    # è¦ªæŠ•ç¨¿ã®URLã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒªãƒ—ãƒ©ã‚¤å›ºæœ‰ã®URLã‚’æ§‹ç¯‰
                    parent_url = tweet.get("url", "")
                    reply_url = ""

                    if parent_url:
                        # è¦ªURLã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼åéƒ¨åˆ†ã‚’æŠ½å‡º
                        username_match = re.search(
                            r"(?:twitter|x)\.com/([^/]+)/status/", parent_url
                        )
                        if username_match:
                            username = username_match.group(1)
                            # ãƒªãƒ—ãƒ©ã‚¤å›ºæœ‰ã®URLã‚’æ§‹ç¯‰
                            reply_url = f"https://x.com/{username}/status/{reply_id}"

                    # è¦ªæŠ•ç¨¿ã®URLã‚’ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒªãƒ—ãƒ©ã‚¤ã«ã‚‚ä¿å­˜
                    add_to_processed_ids_db(
                        notion_client,
                        processed_post_ids_db_id,
                        str(reply_id),
                        "ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒªãƒ—ãƒ©ã‚¤",
                        parent_post_id=current_tweet_id_str,
                        url=reply_url,
                    )
                    registered_ids_map.setdefault("all_processed", set()).add(
                        str(reply_id)
                    )
                else:
                    print(
                        f"â„¹ï¸ ãƒãƒ¼ã‚¸å¯¾è±¡ãƒªãƒ—ãƒ©ã‚¤ {reply_id} ã¯æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ãƒªã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€å‡¦ç†æ¸ˆã¿DBã¸ã®é‡è¤‡ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚"
                    )

        return "SUCCESS"
    except Exception as e:
        print(f"âŒ Notionç™»éŒ²å¤±æ•— (DB: {target_database_id}): Tweet ID {tweet_id_str}")
        print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__} - {e}")
        # traceback.print_exc() # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ãŒå¿…è¦ãªå ´åˆ
        return "FAILED"


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def is_recruit_account(name, bio, config):
    return any(
        k in name or k in bio for k in config.get("filter_keywords_name_bio", [])
    )


def is_recruit_post(text, config):
    return any(k in text for k in config.get("filter_keywords_tweet", []))


def search_accounts(driver, keyword_list):
    results = []
    for keyword in keyword_list:
        search_url = f"https://twitter.com/search?q={keyword}&f=user"
        driver.get(search_url)
        time.sleep(3)

        # âš  æ–°UIæ§‹é€ ã«å¯¾å¿œ
        users = driver.find_elements(
            By.XPATH, "//a[contains(@href, '/')]//div[@dir='auto']/../../.."
        )
        print(f"ğŸ” å€™è£œãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¶æ•°: {len(users)}")

        for user in users:
            try:
                spans = user.find_elements(By.XPATH, ".//span")
                name = ""
                username = ""

                for span in spans:
                    text = span.text.strip()
                    if text.startswith("@"):
                        username = text.replace("@", "")
                    elif not name:
                        name = text

                if username and name:
                    results.append(
                        {
                            "name": name,
                            "username": username,
                            "bio": "",  # ã“ã®æ®µéšã§ã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»é¢ã«é£›ã‚“ã§ã„ãªã„
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºå¤±æ•—: {e}")
                continue

    return results


def merge_replies_with_driver(driver, tweet):
    try:
        driver.get(tweet["url"])
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )

        replies = extract_self_replies(driver, tweet.get("username", ""))
        if not isinstance(replies, list):
            print(
                f"âš ï¸ merge_replies_with_driver() ã§å–å¾—ã—ãŸrepliesãŒä¸æ­£ãªå‹: {type(replies)} â†’ ç©ºãƒªã‚¹ãƒˆã«ç½®æ›"
            )
            replies = []

        tweet_text = tweet.get("text") or ""
        replies = sorted(
            [r for r in replies if r.get("id") and r.get("text")],
            key=lambda x: int(x["id"]),
        )

        existing_chunks = set(tweet_text.strip().split("\n\n"))
        reply_texts = []

        for r in replies:
            # ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ãªã„
            if r.get("images") or r.get("video_posters"):
                print(
                    f"ğŸ›‘ ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ã¾ã›ã‚“: {r['id']}"
                )
                continue

            reply_id = r["id"]
            reply_body = r["text"].strip()
            clean_body = reply_body[:20].replace("\n", " ")
            print(f"ğŸ§µ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå€™è£œ: ID={reply_id} | textå…ˆé ­: {clean_body}")

            if not reply_body:
                continue
            if reply_body in existing_chunks:
                continue
            if r["id"] == tweet["id"]:
                continue

            reply_texts.append(reply_body)
            existing_chunks.add(reply_body)

        if reply_texts:
            tweet["text"] = tweet_text + "\n\n" + "\n\n".join(reply_texts)

    except Exception as e:
        print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå¤±æ•—ï¼ˆ{tweet.get('url', 'ä¸æ˜URL')}ï¼‰: {e}")
    return tweet


def extract_from_search(driver, keywords, max_tweets, name_bio_keywords=None):
    tweets = []
    seen_urls = set()
    seen_users = set()

    for keyword in keywords:
        print(f"ğŸ” è©±é¡Œã®ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ä¸­: {keyword}")
        search_url = f"https://twitter.com/search?q={keyword}&src=typed_query&f=top"
        driver.get(search_url)
        time.sleep(3)

        scroll_count = 0
        max_scrolls = 10
        pause_counter = 0
        pause_threshold = 3
        last_article_count = 0

        while len(tweets) < max_tweets and scroll_count < max_scrolls:
            articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            article_count = len(articles)
            print(f"ğŸ“„ è¡¨ç¤ºä¸­ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {article_count}")
            for article in articles:
                try:
                    # ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åå–å¾—
                    time_el = article.find_element(By.XPATH, ".//time")
                    tweet_href = time_el.find_element(By.XPATH, "..").get_attribute(
                        "href"
                    )
                    tweet_url = (
                        tweet_href
                        if tweet_href.startswith("http")
                        else f"https://x.com{tweet_href}"
                    )
                    if tweet_url in seen_urls:
                        continue
                    seen_urls.add(tweet_url)

                    name_block = article.find_element(
                        By.XPATH, ".//div[@data-testid='User-Name']"
                    )
                    spans = name_block.find_elements(By.XPATH, ".//span")
                    display_name = ""
                    username = ""
                    for s in spans:
                        text = s.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not display_name:
                            display_name = text

                    if not username or username in seen_users:
                        continue
                    seen_users.add(username)

                    # bioãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸å…ˆã«ã‚¢ã‚¯ã‚»ã‚¹
                    if name_bio_keywords:
                        driver.execute_script("window.open('');")
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.get(f"https://twitter.com/{username}")
                        time.sleep(2)
                        try:
                            bio_text = (
                                WebDriverWait(driver, 5)
                                .until(
                                    EC.presence_of_element_located(
                                        (
                                            By.XPATH,
                                            "//div[@data-testid='UserDescription']",
                                        )
                                    )
                                )
                                .text
                            )
                        except:
                            bio_text = ""
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])

                        if not any(
                            k in display_name for k in name_bio_keywords
                        ) and not any(k in bio_text for k in name_bio_keywords):
                            print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿éä¸€è‡´ â†’ ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                            continue

                    # âœ… æ¡ä»¶ã‚’é€šéã—ãŸå ´åˆã®ã¿æŠ•ç¨¿è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦æŠ½å‡º
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(tweet_url)
                    WebDriverWait(driver, 10).until(EC.url_contains("/status/"))

                    try:
                        full_text_el = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located(
                                (By.XPATH, "//div[@data-testid='tweetText']")
                            )
                        )
                        text = full_text_el.text.strip()
                    except Exception as e:
                        print(f"âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")
                        text = ""

                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆå®‰å®šåŒ– + ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« + ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–ï¼‰
                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸å†…ã€ã‚¨ãƒ©ãƒ¼å›é¿ãƒ»å¤šæ®µæ§‹é€ ã«å¯¾å¿œï¼‰
                    date = ""
                    for attempt in range(5):
                        try:
                            driver.execute_script("window.scrollTo(0, 0);")
                            WebDriverWait(driver, 3).until(
                                EC.presence_of_element_located((By.XPATH, "//article"))
                            )
                            time_el = driver.find_element(By.XPATH, "//article//a/time")
                            if time_el:
                                date = time_el.get_attribute("datetime")
                                break
                        except Exception as e:
                            print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—è©¦è¡Œ {attempt+1}/5 å¤±æ•—: {e}")
                            time.sleep(1)

                    if not date:
                        print("âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—ã«å¤±æ•— â†’ ç©ºæ–‡å­—ã§ç¶™ç¶š")

                    # è‡ªãƒªãƒ—ãƒ©ã‚¤å–å¾—ï¼ˆçœç•¥å¯ï¼‰
                    replies = extract_self_replies(driver, username)
                    if replies:
                        reply_texts = [
                            r["text"]
                            for r in replies
                            if "text" in r and r["text"] not in text
                        ]
                        if reply_texts:
                            text += "\n\n" + "\n\n".join(reply_texts)

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])
                    if already_registered(tweet_id):
                        print(f"ğŸš« ç™»éŒ²æ¸ˆ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                        continue

                    tweets.append(
                        {
                            "url": tweet_url,
                            "text": text,
                            "date": date,
                            "id": tweet_id,
                            "username": username,
                            "display_name": display_name,
                        }
                    )

                    print(f"âœ… åé›†: {tweet_url} @{username}")
                    if len(tweets) >= max_tweets:
                        break

                except Exception as e:
                    print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # èª­ã¿è¾¼ã¿åˆ¤å®š
            if article_count == last_article_count:
                pause_counter += 1
                print("ğŸ§Š ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«æ–°ã—ã„æŠ•ç¨¿ãªã—")
                if pause_counter >= pause_threshold:
                    print("ğŸ›‘ æŠ•ç¨¿ãŒå¢—ãˆãªã„ãŸã‚ä¸­æ–­")
                    break
            else:
                pause_counter = 0

            last_article_count = article_count
            scroll_count += 1

    return tweets


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument(
        "--accounts", default="accounts.json", help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰"
    )
    args = parser.parse_args()

    try:
        config = load_config(args.config)
    except FileNotFoundError:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {args.config} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    except json.JSONDecodeError:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {args.config} ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    except Exception as e_conf_load:
        print(
            f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {args.config} ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e_conf_load}"
        )
        return

    try:
        accounts_info = load_config(args.accounts)
    except FileNotFoundError:
        print(f"âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« {args.accounts} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    except json.JSONDecodeError:
        print(
            f"âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« {args.accounts} ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚"
        )
        return
    except Exception as e_acc_load:
        print(
            f"âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« {args.accounts} ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e_acc_load}"
        )
        return

    global TWITTER_EMAIL, TWITTER_USERNAME, TWITTER_PASSWORD
    global EXTRACT_TARGET

    notion_token = config.get("notion_token")
    TWITTER_EMAIL = accounts_info.get("email")
    TWITTER_USERNAME = accounts_info.get("username")
    TWITTER_PASSWORD = accounts_info.get("password")
    EXTRACT_TARGET = config.get("extract_target")
    max_tweets_to_register = config.get("max_tweets", 10)

    required_configs = {
        "Notion Token": notion_token,
        # EXTRACT_TARGET ã¯ mode ã«ã‚ˆã£ã¦ã¯ä¸è¦ãªã®ã§ã€ã“ã“ã§ã¯å¿…é ˆã¨ã—ãªã„
    }
    required_accounts_info = {
        "Twitter Email": TWITTER_EMAIL,
        "Twitter Username": TWITTER_USERNAME,
        "Twitter Password": TWITTER_PASSWORD,
    }

    missing_configs = [key for key, value in required_configs.items() if not value]
    if missing_configs:
        print(
            f"âŒ {args.config}ã«å¿…è¦ãªè¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_configs)}"
        )
        return
    missing_accounts = [
        key for key, value in required_accounts_info.items() if not value
    ]
    if missing_accounts:
        print(
            f"âŒ {args.accounts}ã«å¿…è¦ãªã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_accounts)}"
        )
        return

    mode = config.get("mode", "target_only")
    if mode == "target_only" and not EXTRACT_TARGET:
        print(
            f"âŒ mode 'target_only' ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€{args.config} ã« 'extract_target' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        )
        return

    try:
        notion_client_main = Client(auth=notion_token)
    except Exception as e_notion_client:
        print(f"âŒ Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e_notion_client}")
        return

    registered_ids_map = {}
    db_id_question = config.get("database_id_question")
    db_id_project = config.get("database_id_project")
    db_id_processed_posts = config.get(
        "database_id_processed_posts"
    )  # å‡¦ç†æ¸ˆã¿æŠ•ç¨¿DBã®ID

    all_processed_ids_set = set()  # å…¨ã¦ã®DBã‹ã‚‰é›†ã‚ãŸIDã‚’çµ±åˆã™ã‚‹ã‚»ãƒƒãƒˆ

    if db_id_question:
        question_ids = get_all_registered_ids_from_db(
            db_id_question, notion_client_main, "question"
        )
        all_processed_ids_set.update(question_ids)
        registered_ids_map["question"] = question_ids  # å€‹åˆ¥ã®DBã®IDã‚»ãƒƒãƒˆã‚‚ä¿æŒ
    else:
        print("âš ï¸ config.jsonã« database_id_question ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        registered_ids_map["question"] = set()

    if db_id_project:
        project_ids = get_all_registered_ids_from_db(
            db_id_project, notion_client_main, "project"
        )
        all_processed_ids_set.update(project_ids)
        registered_ids_map["project"] = project_ids  # å€‹åˆ¥ã®DBã®IDã‚»ãƒƒãƒˆã‚‚ä¿æŒ
    else:
        print("âš ï¸ config.jsonã« database_id_project ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        registered_ids_map["project"] = set()

    if db_id_processed_posts:
        processed_ids_from_db = get_all_registered_ids_from_db(
            db_id_processed_posts, notion_client_main, "processed_posts"
        )
        all_processed_ids_set.update(processed_ids_from_db)
        # registered_ids_map["processed_posts"] = processed_ids_from_db # å¿…è¦ãªã‚‰ã“ã‚Œã‚‚ä¿æŒ
    else:
        print(
            "âš ï¸ config.jsonã« database_id_processed_posts (å‡¦ç†æ¸ˆã¿æŠ•ç¨¿ID DB) ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        )
        print(
            "   ã“ã®DBãŒãªã„å ´åˆã€éå»ã«ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ç­‰ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ãŒä¸å®Œå…¨ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        )

    registered_ids_map["all_processed"] = (
        all_processed_ids_set  # çµ±åˆã—ãŸIDã‚»ãƒƒãƒˆã‚’æ ¼ç´
    )

    if not db_id_question and not db_id_project and not db_id_processed_posts:
        print(
            "âŒ config.jsonã«ä¸»è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ID (question, project, processed_posts ã®ã„ãšã‚Œã‹) ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚"
        )
        return
    print(
        f"â„¹ï¸ å…¨DBã®æ—¢å­˜IDãƒªã‚¹ãƒˆæº–å‚™å®Œäº†ã€‚åˆè¨ˆãƒ¦ãƒ‹ãƒ¼ã‚¯IDæ•°: {len(all_processed_ids_set)}"
    )

    driver = None
    try:
        driver = setup_driver()
        login(driver, EXTRACT_TARGET if mode == "target_only" else None)

        chunk_fetch_size = config.get("chunk_fetch_size", 50)
        max_total_urls_to_process = config.get("max_total_urls_to_process", 300)
        iteration_delay_seconds = config.get("iteration_delay_seconds", 60)
        max_fetch_iterations = config.get("max_fetch_iterations", 5)

        all_collected_potential_tweets = []
        processed_tweet_ids_for_upload_loop = set()
        successfully_registered_count = 0
        total_urls_extracted_in_iterations = 0

        for iteration_num in range(max_fetch_iterations):
            if successfully_registered_count >= max_tweets_to_register:
                print(
                    f"ğŸ¯ ç›®æ¨™ç™»éŒ²æ•° ({max_tweets_to_register}ä»¶) ã«é”ã—ãŸãŸã‚ã€å…¨å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"
                )
                break

            print(
                f"\nğŸ”„ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration_num + 1}/{max_fetch_iterations} ã‚’é–‹å§‹ã—ã¾ã™ã€‚"
            )
            print(
                f"   ç¾åœ¨ã®ç™»éŒ²æˆåŠŸæ•°: {successfully_registered_count}/{max_tweets_to_register}"
            )
            print(
                f"   ã“ã‚Œã¾ã§ã«åé›†ã—ãŸURLç·æ•°: {total_urls_extracted_in_iterations}/{max_total_urls_to_process}"
            )
            print(
                f"   ç¾åœ¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿IDæ•° (all_processed_ids_set): {len(all_processed_ids_set)}"
            )

            if total_urls_extracted_in_iterations >= max_total_urls_to_process:
                print(
                    f"ğŸš« ç·åé›†è©¦è¡ŒURLæ•°ãŒä¸Šé™ ({max_total_urls_to_process}) ã«é”ã—ãŸãŸã‚ã€URLåé›†ã‚’åœæ­¢ã—ã¾ã™ã€‚"
                )
                break

            num_urls_to_fetch_this_iteration = min(
                chunk_fetch_size,
                max_total_urls_to_process - total_urls_extracted_in_iterations,
            )

            if num_urls_to_fetch_this_iteration <= 0:
                print(
                    "   ä»Šå›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§åé›†ã™ã‚‹URLæ•°ãŒ0ä»¥ä¸‹ã§ã™ã€‚URLåé›†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                if (
                    iteration_num < max_fetch_iterations - 1
                    and successfully_registered_count < max_tweets_to_register
                ):
                    print(f"   {iteration_delay_seconds}ç§’å¾…æ©Ÿã—ã¾ã™...")
                    time.sleep(iteration_delay_seconds)
                continue

            print(
                f"   ä»Šå›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®URLåé›†ç›®æ¨™: {num_urls_to_fetch_this_iteration}ä»¶"
            )

            current_url_chunk_dicts = []
            if mode == "target_only":
                current_url_chunk_dicts = extract_tweets(
                    driver,
                    EXTRACT_TARGET,
                    num_urls_to_fetch_this_iteration,
                    all_processed_ids_set,
                    config,
                    remaining_needed=max_tweets_to_register
                    - successfully_registered_count,  # æ®‹ã‚Šå¿…è¦æ•°ã‚’æ¸¡ã™
                )
            else:
                print(f"âŒ æœªçŸ¥ã¾ãŸã¯æœªå¯¾å¿œã®modeæŒ‡å®šã§ã™: {mode}")
                break

            if not current_url_chunk_dicts:
                print(
                    f"   ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration_num + 1}: æ–°ã—ã„URLãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                )
                if (
                    iteration_num < max_fetch_iterations - 1
                    and successfully_registered_count < max_tweets_to_register
                ):
                    print(f"   {iteration_delay_seconds}ç§’å¾…æ©Ÿã—ã¾ã™...")
                    time.sleep(iteration_delay_seconds)
                continue

            total_urls_extracted_in_iterations += len(current_url_chunk_dicts)
            # current_url_chunk_dicts.reverse() # extract_tweets ãŒæ–°ã—ã„é †ã«è¿”ã™ã®ã§ã€ã“ã“ã§ã¯ä¸è¦ã‹ã‚‚

            # â˜…â˜…â˜… å¤‰æ›´ç‚¹: extract_and_merge_tweets ã®æˆ»ã‚Šå€¤ã®å—ã‘å–ã‚Šæ–¹ã‚’å¤‰ãˆã‚‹ â˜…â˜…â˜…
            newly_processed_tweets_chunk, cycle_processed_ids = (
                extract_and_merge_tweets(
                    driver,
                    current_url_chunk_dicts,
                    max_tweets_to_register,
                    notion_client_main,
                    config,
                    registered_ids_map,  # ã“ã“ã« all_processed_ids_set ãŒå«ã¾ã‚Œã¦ã„ã‚‹
                    current_success_count=successfully_registered_count,  # ç¾åœ¨ã®ç™»éŒ²æˆåŠŸæ•°ã‚’è¿½åŠ 
                )
            )
            # â˜…â˜…â˜… ã“ã“ã¾ã§ â˜…â˜…â˜…

            # â˜…â˜…â˜… ä¿®æ­£: extract_and_merge_tweets ã§å‡¦ç†æ¸ˆã¿ã¨ã•ã‚ŒãŸIDã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚»ãƒƒãƒˆã«ã™ãè¿½åŠ ã—ãªã„ â˜…â˜…â˜…
            # if cycle_processed_ids: # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã¾ãŸã¯å‰Šé™¤
            #     all_processed_ids_set.update(cycle_processed_ids)
            #     # registered_ids_map["all_processed"] ã‚‚æ›´æ–° (åŒä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãªã®ã§è‡ªå‹•çš„ã«æ›´æ–°ã•ã‚Œã‚‹ã¯ãšã ãŒå¿µã®ãŸã‚)
            #     registered_ids_map["all_processed"] = all_processed_ids_set
            #     print(
            #         f"   DEBUG main: {len(cycle_processed_ids)} IDs from extract_and_merge_tweets added to all_processed_ids_set. New total: {len(all_processed_ids_set)}"
            #     )
            # â˜…â˜…â˜… ã“ã“ã¾ã§ â˜…â˜…â˜…

            if newly_processed_tweets_chunk:
                # æ—¢å­˜ã® all_collected_potential_tweets ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«è¿½åŠ 
                # ï¼ˆãŸã ã—ã€IDãƒ™ãƒ¼ã‚¹ã§ã‚ˆã‚Šå³å¯†ãªé‡è¤‡æ’é™¤ãŒæœ›ã¾ã—ã„å ´åˆã‚‚ã‚ã‚‹ï¼‰
                # ç¾åœ¨ã¯å˜ç´”ã«è¿½åŠ ã—ã¦ã„ã‚‹ãŒã€extract_tweetsã§åé›†ã—ãŸURLã®æ™‚ç‚¹ã§ã‚ã‚‹ç¨‹åº¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ãªã£ã¦ã„ã‚‹æƒ³å®š
                unique_new_tweets = [
                    tweet
                    for tweet in newly_processed_tweets_chunk
                    if tweet.get("id") not in processed_tweet_ids_for_upload_loop
                ]
                all_collected_potential_tweets.extend(unique_new_tweets)
                print(
                    f"   newly_processed_tweets_chunk ({len(newly_processed_tweets_chunk)}ä»¶) ã®ã†ã¡ã€unique {len(unique_new_tweets)} ä»¶ã‚’å‡¦ç†å€™è£œã«è¿½åŠ ã€‚"
                    f"ç¾åœ¨ã®å‡¦ç†å€™è£œç·æ•°: {len(all_collected_potential_tweets)}ä»¶ã€‚"
                )

                # å‡¦ç†å€™è£œãƒªã‚¹ãƒˆã‚’æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã€é•·ã•ã‚’ç›®æ¨™ç™»éŒ²ä»¶æ•°ã¾ã§ã«åˆ¶é™
                all_collected_potential_tweets.sort(
                    key=lambda x: (
                        int(x["id"])
                        if x.get("id") and str(x["id"]).isdigit()
                        else float("-inf")
                    ),
                    reverse=True,  # æ–°ã—ã„ã‚‚ã®ï¼ˆIDãŒå¤§ãã„ã‚‚ã®ï¼‰ãŒå…ˆé ­
                )
                if len(all_collected_potential_tweets) > max_tweets_to_register:
                    all_collected_potential_tweets = all_collected_potential_tweets[
                        :max_tweets_to_register
                    ]
                    print(
                        f"   å‡¦ç†å€™è£œãƒªã‚¹ãƒˆã‚’æœ€æ–°ã® {len(all_collected_potential_tweets)} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸï¼ˆç›®æ¨™ç™»éŒ²æ•°: {max_tweets_to_register}ï¼‰ã€‚"
                    )

            print(
                f"\nğŸ“Š ç¾åœ¨ã®å‡¦ç†å€™è£œã®åˆè¨ˆãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(all_collected_potential_tweets)} ä»¶ (ã†ã¡ä»Šå›æ–°è¦è©³ç´°åŒ–: {len(newly_processed_tweets_chunk) if newly_processed_tweets_chunk else 0}ä»¶)"
            )

            processed_this_iteration_for_log = 0
            # all_collected_potential_tweets ã¯æ—¢ã«æ–°ã—ã„é †ã«ãªã£ã¦ã„ã‚‹ã¯ãš
            for tweet_data in all_collected_potential_tweets:
                if successfully_registered_count >= max_tweets_to_register:
                    break

                tweet_id_for_check = tweet_data.get("id")
                if tweet_id_for_check in processed_tweet_ids_for_upload_loop:
                    continue  # ã“ã® main ãƒ«ãƒ¼ãƒ—ã® upload è©¦è¡Œæ¸ˆã¿ãƒªã‚¹ãƒˆã§ãƒã‚§ãƒƒã‚¯

                # upload_to_notion ã«æ¸¡ã™å‰ã«ã€å†åº¦ã‚°ãƒ­ãƒ¼ãƒãƒ«å‡¦ç†æ¸ˆã¿ã‚»ãƒƒãƒˆã§ç¢ºèª
                # (extract_and_merge_tweets ã§ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã®ã¯ãšã ãŒã€å¿µã«ã¯å¿µã‚’)
                if tweet_id_for_check in all_processed_ids_set:
                    print(
                        f"   DEBUG main: ID {tweet_id_for_check} is in all_processed_ids_set before upload_to_notion. Skipping upload loop for this ID."
                    )
                    processed_tweet_ids_for_upload_loop.add(
                        tweet_id_for_check
                    )  # uploadãƒ«ãƒ¼ãƒ—ã§ã¯å‡¦ç†æ¸ˆã¿ã¨ã™ã‚‹
                    continue

                processed_tweet_ids_for_upload_loop.add(tweet_id_for_check)
                processed_this_iteration_for_log += 1

                print(
                    f"\nğŸŒ€ Notionç™»éŒ²è©¦è¡Œ: {len(processed_tweet_ids_for_upload_loop)}/{len(all_collected_potential_tweets)} ä»¶ç›®å€™è£œ (ID: {tweet_id_for_check}) (ç™»éŒ²æˆåŠŸ: {successfully_registered_count}/{max_tweets_to_register})"
                )

                tweet_data_for_upload = tweet_data.copy()
                tweet_data_for_upload.pop("article_element", None)

                upload_status = upload_to_notion(
                    tweet_data_for_upload,
                    config,
                    notion_client_main,
                    registered_ids_map,  # ã“ã“ã« all_processed_ids_set ãŒå«ã¾ã‚Œã¦ã„ã‚‹
                    db_id_processed_posts,
                )

                # â˜…â˜…â˜… upload_status ã«å¿œã˜ã¦ all_processed_ids_set ã‚’æ›´æ–° â˜…â˜…â˜…
                current_tweet_id_str_for_set = str(tweet_data.get("id"))
                merged_ids_for_set = tweet_data.get("merged_reply_ids", [])
                if isinstance(merged_ids_for_set, str):  # æ–‡å­—åˆ—ãªã‚‰ãƒªã‚¹ãƒˆã«å¤‰æ›
                    merged_ids_for_set = [
                        m_id.strip()
                        for m_id in merged_ids_for_set.split(",")
                        if m_id.strip().isdigit()
                    ]

                if upload_status == "SUCCESS":
                    successfully_registered_count += 1
                    all_processed_ids_set.add(current_tweet_id_str_for_set)
                    for m_id in merged_ids_for_set:
                        all_processed_ids_set.add(str(m_id))
                elif upload_status == "SKIPPED_REGISTERED":
                    # print(f"â„¹ï¸ Tweet ID {tweet_data.get('id')} ã¯ç™»éŒ²æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚") # upload_to_notionå†…ã§ãƒ­ã‚°å‡ºåŠ›æ¸ˆã¿
                    all_processed_ids_set.add(
                        current_tweet_id_str_for_set
                    )  # å¿µã®ãŸã‚è¿½åŠ 
                    for m_id in merged_ids_for_set:
                        all_processed_ids_set.add(str(m_id))
                elif upload_status == "SKIPPED_CLASSIFICATION":
                    # print(f"â„¹ï¸ Tweet ID {tweet_data.get('id')} ã¯åˆ†é¡ã«ã‚ˆã‚Šç™»éŒ²å¯¾è±¡å¤–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚") # upload_to_notionå†…ã§ãƒ­ã‚°å‡ºåŠ›æ¸ˆã¿
                    all_processed_ids_set.add(
                        current_tweet_id_str_for_set
                    )  # åˆ†é¡ã‚¹ã‚­ãƒƒãƒ—ã‚‚å‡¦ç†æ¸ˆã¿ã¨ã¿ãªã™
                    for m_id in merged_ids_for_set:
                        all_processed_ids_set.add(str(m_id))
                elif upload_status == "FAILED_CONFIG":
                    # print(f"âš ï¸ Tweet ID {tweet_data.get('id')} ã¯DBè¨­å®šä¸å‚™ã€‚config.jsonã‚’ç¢ºèªã€‚") # upload_to_notionå†…ã§ãƒ­ã‚°å‡ºåŠ›æ¸ˆã¿
                    all_processed_ids_set.add(
                        current_tweet_id_str_for_set
                    )  # è¨­å®šä¸å‚™ã§ã‚‚å†è©¦è¡Œã—ãªã„ã‚ˆã†ã«å‡¦ç†æ¸ˆã¿ã¨ã™ã‚‹
                elif upload_status == "FAILED":
                    # print(f"âš ï¸ Tweet ID {tweet_data.get('id')} ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸã€‚") # upload_to_notionå†…ã§ãƒ­ã‚°å‡ºåŠ›æ¸ˆã¿
                    all_processed_ids_set.add(
                        current_tweet_id_str_for_set
                    )  # å¤±æ•—ã—ãŸå ´åˆã‚‚å‡¦ç†æ¸ˆã¿ã¨ã™ã‚‹

                # registered_ids_map["all_processed"] ã‚‚æ›´æ–°
                registered_ids_map["all_processed"] = all_processed_ids_set
                # â˜…â˜…â˜… ã“ã“ã¾ã§ â˜…â˜…â˜…

                temp_files_after_upload = ["tweet_for_gpt.json", "gpt_output.json"]
                for temp_file_au in temp_files_after_upload:
                    if os.path.exists(temp_file_au):
                        try:
                            os.remove(temp_file_au)
                        except:
                            pass

            # all_collected_potential_tweets ã‹ã‚‰ processed_tweet_ids_for_upload_loop ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’å‰Šé™¤ã™ã‚‹
            # ãŸã ã—ã€Notionç™»éŒ²æˆåŠŸã—ãŸã‚‚ã®ã ã‘ã‚’å‰Šé™¤ã™ã‚‹ã‹ã€è©¦è¡Œã—ãŸã‚‚ã®ã¯å…¨ã¦å‰Šé™¤ã™ã‚‹ã‹ã¯ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã‚‹
            # ã“ã“ã§ã¯ã€uploadãƒ«ãƒ¼ãƒ—ã§è©¦è¡Œã—ãŸã‚‚ã®ã¯ all_collected_potential_tweets ã‹ã‚‰ã¯ä¸€æ—¦å‰Šé™¤ã—ãªã„ã§ãŠã
            # (æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å†åº¦è©³ç´°åŒ–ã•ã‚Œã‚‹ã“ã¨ã¯ãªã„ãŒã€ãƒªã‚¹ãƒˆã«æ®‹ã£ã¦ã„ã¦ã‚‚å®³ã¯å°‘ãªã„)
            # ã‚‚ã—ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æ°—ã«ã™ã‚‹ãªã‚‰ã€ã“ã“ã§å‰Šé™¤ã™ã‚‹ã€‚
            # all_collected_potential_tweets = [
            #     t for t in all_collected_potential_tweets if t["id"] not in processed_tweet_ids_for_upload_loop
            # ]

            print(
                f"   ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration_num + 1}: Notionç™»éŒ²ãƒ«ãƒ¼ãƒ—ã§ {processed_this_iteration_for_log} ä»¶ã®å€™è£œã‚’å‡¦ç†ã—ã¾ã—ãŸã€‚"
            )

            if successfully_registered_count >= max_tweets_to_register:
                print(
                    f"ğŸ¯ ç›®æ¨™ç™»éŒ²æ•° ({max_tweets_to_register}ä»¶) ã«é”ã—ãŸãŸã‚ã€å…¨å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"
                )
                break

            if iteration_num < max_fetch_iterations - 1:
                if total_urls_extracted_in_iterations < max_total_urls_to_process:
                    print(
                        f"\nã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {iteration_num + 1} çµ‚äº†ã€‚{iteration_delay_seconds}ç§’å¾…æ©Ÿã—ã¾ã™..."
                    )
                    time.sleep(iteration_delay_seconds)
                else:
                    print(
                        f"\nç·åé›†è©¦è¡ŒURLæ•°ãŒä¸Šé™ ({max_total_urls_to_process}) ã«é”ã—ãŸãŸã‚ã€ã“ã‚Œä»¥ä¸Šã®URLåé›†ã¯è¡Œã„ã¾ã›ã‚“ã€‚"
                    )
            else:
                print(
                    f"\næœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•° ({max_fetch_iterations}) ã«é”ã—ã¾ã—ãŸã€‚"
                )

        if successfully_registered_count < max_tweets_to_register:
            print(
                f"\nâš ï¸ å…¨ã¦ã®å‡¦ç†ã‚’è©¦ã¿ã¾ã—ãŸãŒã€ç›®æ¨™ç™»éŒ²æ•° ({max_tweets_to_register}ä»¶) ã«é”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                f"å®Ÿéš›ã«ç™»éŒ²ã•ã‚ŒãŸã®ã¯ {successfully_registered_count}ä»¶ã§ã™ã€‚"
            )
        else:
            print(
                f"\nâœ… ç›®æ¨™ã® {successfully_registered_count} ä»¶ã®ç™»éŒ²ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
            )

    except Exception as e_main:
        print(
            f"âŒ mainå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e_main).__name__} - {e_main}"
        )
        print(traceback.format_exc())
    finally:
        if driver:
            driver.quit()
            print("ğŸšª ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")

        temp_files_to_remove = ["tweet_for_gpt.json", "gpt_output.json"]
        for temp_file in temp_files_to_remove:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    print(f"ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_file}")
                except Exception as e_remove_temp:
                    print(f"âš ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å¤±æ•— ({temp_file}): {e_remove_temp}")

        temp_dirs_to_remove = ["temp_posters", "temp_ocr_images"]
        for temp_dir in temp_dirs_to_remove:
            if os.path.exists(temp_dir):
                try:
                    shutil.rmtree(
                        temp_dir, ignore_errors=True
                    )  # ignore_errors=True ã‚’è¿½åŠ 
                    print(f"ğŸ—‘ï¸ ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤: {temp_dir}")
                except Exception as e_remove_dir:
                    print(f"âš ï¸ ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤å¤±æ•— ({temp_dir}): {e_remove_dir}")

    print("âœ… å…¨æŠ•ç¨¿ã®å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
